{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\helen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\helen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\helen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import ast\n",
    "from surprise import Dataset, Reader, KNNBasic\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from surprise.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split as skl_train_test_split\n",
    "from datetime import datetime\n",
    "from gensim.models.doc2vec import Doc2Vec,TaggedDocument\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "\n",
    "df_business = pd.read_csv('yelp_academic_dataset_business.csv')\n",
    "df_checkin = pd.read_csv('yelp_academic_dataset_checkin.csv')\n",
    "df_review=pd.read_csv('yelp_academic_dataset_review.csv')\n",
    "df_tip=pd.read_csv('yelp_academic_dataset_tip.csv')\n",
    "df_user=pd.read_csv('yelp_academic_dataset_user.csv')\n",
    "\n",
    "# print(df_business.columns)\n",
    "# print(df_checkin.columns)\n",
    "# print(df_review.columns)\n",
    "# print(df_tip.columns)\n",
    "# print(df_user.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_rating(cosine_similarity):\n",
    "    '''\n",
    "    Maps a consine similarity score to a rating from 1 to 5\n",
    "    '''\n",
    "    return 1 + 4 * ((cosine_similarity + 1) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "\n",
    "def load_data():\n",
    "    # df_business = pd.read_csv('yelp_academic_dataset_business.csv')\n",
    "    # df_review=pd.read_csv('yelp_academic_dataset_review.csv')\n",
    "    # df_user=pd.read_csv('yelp_academic_dataset_user.csv')\n",
    "    \n",
    "    df_business_filadelfia = df_business[(df_business['city'] == 'Philadelphia') & (df_business['categories'].str.contains('Restaurants', na=False)) & (df_business['is_open']==1)].reset_index(drop=True)\n",
    "    df_business_filadelfia=df_business_filadelfia[['business_id', 'name', 'stars', 'review_count', 'attributes', 'categories', 'hours']].reset_index(drop=True)\n",
    "    df_business_filadelfia = df_business_filadelfia.dropna().reset_index(drop=True)\n",
    "\n",
    "    df_review_filadelfia = df_review[df_review['business_id'].isin(df_business_filadelfia['business_id'])]\n",
    "    df_review_filadelfia=df_review_filadelfia[['review_id', 'user_id', 'business_id', 'stars', 'text', 'date']]\n",
    "    df_review_filadelfia['liked'] = (df_review_filadelfia['stars'] > 3).astype(int)\n",
    "    df_review_filadelfia_profiles = df_review_filadelfia[df_review_filadelfia['liked'] == 1].reset_index(drop=True)\n",
    "    df_review_filadelfia = df_review_filadelfia.dropna().reset_index(drop=True)\n",
    "\n",
    "    df_user_filadelfia = df_user[df_user['user_id'].isin(df_review_filadelfia['user_id'])]\n",
    "    df_user_filadelfia=df_user_filadelfia[['user_id', 'name', 'review_count', 'yelping_since', 'elite', 'average_stars']].reset_index(drop=True)\n",
    "    df_user_filadelfia = df_user_filadelfia.dropna().reset_index(drop=True)\n",
    "\n",
    "    # counts\n",
    "    user_counts = df_review_filadelfia['user_id'].value_counts()\n",
    "    restaurant_counts = df_review_filadelfia['business_id'].value_counts()\n",
    "\n",
    "    # creating filters for users and restaurants with 5+ reviews\n",
    "    users_with_5_plus_reviews = user_counts[user_counts >= 5].index\n",
    "    restaurants_with_5_plus_reviews = restaurant_counts[restaurant_counts >= 5].index\n",
    "\n",
    "    return df_business_filadelfia,df_review_filadelfia,df_user_filadelfia, users_with_5_plus_reviews, restaurants_with_5_plus_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    words = text.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def stem_text(text):\n",
    "    words = text.split()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(stemmed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processamento\n",
    "def pre_processing(data_review,method):\n",
    "    if method == 'with lemma':\n",
    "\n",
    "        data_review['text'] = data_review['text'].apply(lemmatize_text)\n",
    "\n",
    "    \n",
    "    else: \n",
    "        data_review['text'] = data_review['text'].apply(stem_text)\n",
    "\n",
    "    return data_review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering\n",
    "def feature_engineering(data_reviews, method, components=8):\n",
    "    if method == 'bag of words':\n",
    "        vectorizer = CountVectorizer(max_df=0.95, min_df=2)\n",
    "        matrix = vectorizer.fit_transform(data_reviews['text'])\n",
    "        matrix = matrix.toarray()\n",
    "        components = matrix.shape[1]\n",
    "        # feature_names = vectorizer.get_feature_names_out()\n",
    "        # df = pd.DataFrame(matrix, columns=feature_names)\n",
    "   \n",
    "    elif method =='lda':\n",
    "        count_vectorizer = CountVectorizer(max_df=0.95, min_df=2)\n",
    "        count_matrix = count_vectorizer.fit_transform(data_reviews['text'])\n",
    "        lda_model = LatentDirichletAllocation(n_components=components, random_state=42)\n",
    "        matrix = lda_model.fit_transform(count_matrix)\n",
    "        # df = pd.DataFrame(matrix, columns=[f'Topic_{i}' for i in range(lda_model.n_components)])\n",
    "    \n",
    "    elif method =='lsa':\n",
    "        vectorizer = CountVectorizer()\n",
    "        count_matrix = vectorizer.fit_transform(data_reviews['text'])\n",
    "        lsa_model = TruncatedSVD(n_components=components)\n",
    "        matrix = lsa_model.fit_transform(count_matrix)\n",
    "\n",
    "    elif method == 'doc2vec':\n",
    "        # preproces the documents, and create TaggedDocuments\n",
    "        tagged_data = [TaggedDocument(words=word_tokenize(doc.lower()),\n",
    "                                    tags=[str(i)]) for i,\n",
    "                    doc in enumerate(data_reviews['text'])]\n",
    "\n",
    "        # Doc2vec model\n",
    "        model = Doc2Vec(vector_size=components,\n",
    "                        min_count=2, epochs=50)\n",
    "        model.build_vocab(tagged_data)\n",
    "        model.train(tagged_data,\n",
    "                    total_examples=model.corpus_count,\n",
    "                    epochs=model.epochs)\n",
    "\n",
    "        # document vectors\n",
    "        matrix = [model.infer_vector(\n",
    "            word_tokenize(doc.lower())) for doc in data_reviews['text']]\n",
    "\n",
    "    return matrix, components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_business(df_business_filadelfia):\n",
    "    df_business_filadelfia = df_business_filadelfia[['business_id', 'name', 'stars', 'review_count','attributes', 'categories', 'hours']]\n",
    "\n",
    "\n",
    "    #variavel horario\n",
    "    df_business_filadelfia['hours'] = df_business_filadelfia['hours'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else {})\n",
    "\n",
    "    # Crie colunas separadas para cada dia da semana\n",
    "    dias_da_semana = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "    for dia in dias_da_semana:\n",
    "        df_business_filadelfia[dia] = df_business_filadelfia['hours'].apply(lambda x: x.get(dia, None))\n",
    "\n",
    "    # Remova a coluna original 'hours' se não for mais necessária\n",
    "    df_business_filadelfia.drop(columns=['hours'], inplace=True)\n",
    "\n",
    "\n",
    "    def categorize_hours(hours):\n",
    "\n",
    "        if hours == None:\n",
    "            return 0 #'Fechado'\n",
    "        \n",
    "        else:\n",
    "            start_time, end_time = hours.split('-')\n",
    "            start_hour = int(start_time.split(':')[0])\n",
    "            end_hour = int(end_time.split(':')[0])\n",
    "            \n",
    "            if end_hour <= 12:\n",
    "                return 1 #'Manhã'\n",
    "            elif 12 < start_hour and end_hour<=15:\n",
    "                return 2 #'Almoço'\n",
    "            elif start_hour > 15 and end_hour < 19:\n",
    "                return 3 #'Tarde'\n",
    "            elif start_hour>=19:\n",
    "                return 4 #'Noite'\n",
    "            else:\n",
    "                return 5 #'Dia todo'\n",
    "\n",
    "    # Aplicar a função de categorização a cada coluna de dia da semana\n",
    "    for day in ['Monday', 'Tuesday', 'Wednesday','Thursday', 'Friday', 'Saturday', 'Sunday']:\n",
    "        df_business_filadelfia[day] = df_business_filadelfia[day].apply(categorize_hours)\n",
    "\n",
    "\n",
    "    #variavel categoria\n",
    "    df_business_filadelfia['Food'] = 0\n",
    "    df_business_filadelfia['Nightlife'] = 0\n",
    "    df_business_filadelfia['Bars'] = 0\n",
    "    df_business_filadelfia['Sandwiches'] = 0\n",
    "    df_business_filadelfia['American (New)'] = 0\n",
    "    df_business_filadelfia['Pizza'] = 0\n",
    "    df_business_filadelfia['Breakfast & Brunch'] = 0\n",
    "    df_business_filadelfia['American (Traditional)'] = 0\n",
    "    df_business_filadelfia['Coffee & Tea'] = 0\n",
    "\n",
    "    for index, row in df_business_filadelfia.iterrows():\n",
    "        categories = row['categories']\n",
    "\n",
    "        if 'Food' in categories:\n",
    "            df_business_filadelfia.at[index, 'Food'] = 1\n",
    "\n",
    "        if 'Nightlife' in categories:\n",
    "            df_business_filadelfia.at[index, 'Nightlife'] = 1\n",
    "\n",
    "        if 'Bars' in categories:\n",
    "            df_business_filadelfia.at[index, 'Bars'] = 1\n",
    "\n",
    "        if 'Sandwiches' in categories:\n",
    "            df_business_filadelfia.at[index, 'Sandwiches'] = 1\n",
    "\n",
    "        if 'American (New)' in categories:\n",
    "            df_business_filadelfia.at[index, 'American (New)'] = 1\n",
    "        \n",
    "        if 'Pizza' in categories:\n",
    "            df_business_filadelfia.at[index, 'Pizza'] = 1\n",
    "\n",
    "        if 'Breakfast & Brunch' in categories:\n",
    "            df_business_filadelfia.at[index, 'Breakfast & Brunch'] = 1\n",
    "\n",
    "        if 'American (Traditional)' in categories:\n",
    "            df_business_filadelfia.at[index, 'American (Traditional)'] = 1\n",
    "\n",
    "        if 'Coffee & Tea' in categories:\n",
    "            df_business_filadelfia.at[index, 'Coffee & Tea'] = 1\n",
    "\n",
    "\n",
    "\n",
    "    #variavel atributos\n",
    "\n",
    "    df_business_filadelfia['RestaurantsTakeOut'] = 0\n",
    "    df_business_filadelfia['BusinessAcceptsCreditCards'] = 0\n",
    "    df_business_filadelfia['RestaurantsDelivery'] = 0\n",
    "    df_business_filadelfia['RestaurantsAttire_casual'] = 0\n",
    "    df_business_filadelfia['HasTV'] = 0\n",
    "    df_business_filadelfia['RestaurantsGoodForGroups'] = 0\n",
    "    df_business_filadelfia['BikeParking'] = 0\n",
    "    df_business_filadelfia['BusinessParking_street'] = 0\n",
    "    df_business_filadelfia['GoodForKids'] = 0\n",
    "\n",
    "\n",
    "    df_business_filadelfia['attributes'] = df_business_filadelfia['attributes'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else {})\n",
    "\n",
    "    df_atributos = pd.json_normalize(df_business_filadelfia['attributes'])\n",
    "    # Normalizar a coluna 'BusinessParking' para expandir as sub-chaves\n",
    "    if 'BusinessParking' in df_atributos:\n",
    "        df_parking = pd.json_normalize(df_atributos['BusinessParking'].dropna().apply(ast.literal_eval), sep='_')\n",
    "        df_parking.columns = ['BusinessParking_garage', 'BusinessParking_street', 'BusinessParking_validated', 'BusinessParking_lot', 'BusinessParking_valet']\n",
    "        # Juntar o df_parking ao df_atributos\n",
    "        df_atributos = df_atributos.drop(columns=['BusinessParking']).join(df_parking, how='left')\n",
    "\n",
    "    if \"Ambience\" in df_atributos:\n",
    "        df_ambience = pd.json_normalize(df_atributos['Ambience'].dropna().apply(ast.literal_eval), sep='_')\n",
    "        df_ambience.columns = [f\"Ambience_{col}\" for col in df_ambience.columns]\n",
    "        # Juntar o df_ambience ao df_atributos\n",
    "        df_atributos = df_atributos.drop(columns=['Ambience']).join(df_ambience, how='left')\n",
    "\n",
    "    if \"GoodForMeal\" in df_atributos:\n",
    "        df_GoodForMeal = pd.json_normalize(df_atributos['GoodForMeal'].dropna().apply(ast.literal_eval), sep='_')\n",
    "        df_GoodForMeal.columns = [f\"GoodForMeal_{col}\" for col in df_GoodForMeal.columns]\n",
    "        # Juntar o df_GoodForMeal ao df_atributos\n",
    "        df_atributos = df_atributos.drop(columns=['GoodForMeal']).join(df_GoodForMeal, how='left')\n",
    "\n",
    "    if \"Music\" in df_atributos:\n",
    "        df_Music = pd.json_normalize(df_atributos['Music'].dropna().apply(ast.literal_eval), sep='_')\n",
    "        df_Music.columns = [f\"Music_{col}\" for col in df_Music.columns]\n",
    "        # Juntar o df_Music ao df_atributos\n",
    "        df_atributos = df_atributos.drop(columns=['Music']).join(df_Music, how='left')\n",
    "\n",
    "    if \"BestNights\" in df_atributos:\n",
    "        df_BestNights = pd.json_normalize(df_atributos['BestNights'].dropna().apply(ast.literal_eval), sep='_')\n",
    "        df_BestNights.columns = [f\"BestNights_{col}\" for col in df_BestNights.columns]\n",
    "        # Juntar o df_BestNights ao df_atributos\n",
    "        df_atributos = df_atributos.drop(columns=['BestNights']).join(df_BestNights, how='left')\n",
    "\n",
    "    if \"DietaryRestrictions\" in df_atributos:\n",
    "        df_DietaryRestrictions = pd.json_normalize(df_atributos['DietaryRestrictions'].dropna().apply(ast.literal_eval), sep='_')\n",
    "        df_DietaryRestrictions.columns = [f\"DietaryRestrictions_{col}\" for col in df_DietaryRestrictions.columns]\n",
    "        # Juntar o df_DietaryRestrictions ao df_atributos\n",
    "        df_atributos = df_atributos.drop(columns=['DietaryRestrictions']).join(df_DietaryRestrictions, how='left')\n",
    "\n",
    "    \n",
    "    df_atributos = df_atributos.applymap(lambda x: x.replace(\"u'\", \"\").replace(\"'\", \"\") if isinstance(x, str) else x)\n",
    "    def create_price_range_columns(row):\n",
    "        price_ranges = ['1', '2', '3', '4']\n",
    "        for price in price_ranges:\n",
    "            column_name = f'RestaurantsPriceRange2_{price}'\n",
    "            if row['RestaurantsPriceRange2'] == price:\n",
    "                row[column_name] = 'True'\n",
    "            elif row['RestaurantsPriceRange2'] == 'False':\n",
    "                row[column_name] = 'False'\n",
    "            else:\n",
    "                row[column_name] = 'False'\n",
    "        return row\n",
    "\n",
    "    # Aplicar a função linha por linha\n",
    "    df_atributos = df_atributos.apply(create_price_range_columns, axis=1)\n",
    "    df_atributos=df_atributos.drop('RestaurantsPriceRange2',axis=1)\n",
    "\n",
    "    def alchool_columns(row):\n",
    "        types = ['full_bar','beer_and_wine']\n",
    "        for t in types:\n",
    "            column_name = f'Alcohol_{t}'\n",
    "            if row['Alcohol'] == t:\n",
    "                row[column_name] = 'True'\n",
    "            elif row['Alcohol'] == 'False':\n",
    "                row[column_name] = 'False'\n",
    "            else:\n",
    "                row[column_name] = 'False'\n",
    "        return row\n",
    "\n",
    "    df_atributos = df_atributos.apply(alchool_columns, axis=1)\n",
    "    df_atributos=df_atributos.drop('Alcohol',axis=1)\n",
    "\n",
    "\n",
    "    def wifi_columns(row):\n",
    "        types = ['free','paid']\n",
    "        for t in types:\n",
    "            column_name = f'WiFi_{t}'\n",
    "            if row['WiFi'] == t:\n",
    "                row[column_name] = 'True'\n",
    "            elif row['WiFi'] == 'False':\n",
    "                row[column_name] = 'False'\n",
    "            else:\n",
    "                row[column_name] = 'False'\n",
    "        return row\n",
    "\n",
    "    df_atributos = df_atributos.apply(wifi_columns, axis=1)\n",
    "    df_atributos=df_atributos.drop('WiFi',axis=1)\n",
    "\n",
    "\n",
    "    def attire_columns(row):\n",
    "        types = ['casual', 'dressy', 'formal']\n",
    "        for t in types:\n",
    "            column_name = f'RestaurantsAttire_{t}'\n",
    "            if row['RestaurantsAttire'] == t:\n",
    "                row[column_name] = 'True'\n",
    "            elif row['RestaurantsAttire'] == 'False':\n",
    "                row[column_name] = 'False'\n",
    "            else:\n",
    "                row[column_name] = 'False'\n",
    "        return row\n",
    "\n",
    "    df_atributos = df_atributos.apply(attire_columns, axis=1)\n",
    "    df_atributos=df_atributos.drop('RestaurantsAttire',axis=1)\n",
    "\n",
    "    def noise_columns(row):\n",
    "        types = ['average', 'quiet', 'loud', 'very_loud']\n",
    "        for t in types:\n",
    "            column_name = f'NoiseLevel_{t}'\n",
    "            if row['NoiseLevel'] == t:\n",
    "                row[column_name] = 'True'\n",
    "            elif row['NoiseLevel'] == 'False':\n",
    "                row[column_name] = 'False'\n",
    "            else:\n",
    "                row[column_name] = 'False'\n",
    "        return row\n",
    "\n",
    "    df_atributos = df_atributos.apply(noise_columns, axis=1)\n",
    "    df_atributos=df_atributos.drop('NoiseLevel',axis=1)\n",
    "\n",
    "    def ages_columns(row):\n",
    "        types = ['21plus', 'allages']\n",
    "        for t in types:\n",
    "            column_name = f'AgesAllowed_{t}'\n",
    "            if row['AgesAllowed'] == t:\n",
    "                row[column_name] = 'True'\n",
    "            elif row['AgesAllowed'] == 'False':\n",
    "                row[column_name] = 'False'\n",
    "            else:\n",
    "                row[column_name] = 'False'\n",
    "        return row\n",
    "\n",
    "    df_atributos = df_atributos.apply(ages_columns, axis=1)\n",
    "    df_atributos=df_atributos.drop('AgesAllowed',axis=1)\n",
    "\n",
    "    df_atributos = df_atributos.replace('None','False')\n",
    "    df_atributos = df_atributos.replace( np.nan,'False')\n",
    "    df_atributos = df_atributos.replace('none','False')\n",
    "    df_atributos = df_atributos.replace('no','False')\n",
    "    df_atributos = df_atributos.replace('yes','True')\n",
    "    df_atributos['Smoking'] = df_atributos['Smoking'].replace('outdoor','True')\n",
    "    df_atributos['BYOBCorkage'] = df_atributos['BYOBCorkage'].replace('yes_free','True')\n",
    "    df_atributos['BYOBCorkage'] = df_atributos['BYOBCorkage'].replace('yes_corkage','True')\n",
    "\n",
    "    df_atributos['business_id'] = df_business_filadelfia['business_id']\n",
    "\n",
    "\n",
    "    for index, row in df_atributos.iterrows():\n",
    "        # attributes = row['attributes']\n",
    "        if row['RestaurantsTakeOut']=='True':\n",
    "            df_business_filadelfia.at[index, 'RestaurantsTakeOut'] = 1\n",
    "\n",
    "        if row['BusinessAcceptsCreditCards']=='True':\n",
    "            df_business_filadelfia.at[index, 'BusinessAcceptsCreditCards'] = 1\n",
    "\n",
    "        if row['RestaurantsDelivery']=='True':\n",
    "            df_business_filadelfia.at[index, 'RestaurantsDelivery'] = 1\n",
    "\n",
    "        if row['RestaurantsAttire_casual']=='True':\n",
    "            df_business_filadelfia.at[index, 'RestaurantsAttire_casual'] = 1\n",
    "\n",
    "        if row['HasTV']=='True':\n",
    "            df_business_filadelfia.at[index, 'HasTV'] = 1\n",
    "\n",
    "        if row['RestaurantsGoodForGroups']=='True':\n",
    "            df_business_filadelfia.at[index, 'RestaurantsGoodForGroups'] = 1\n",
    "\n",
    "        if row['BikeParking']=='True':\n",
    "            df_business_filadelfia.at[index, 'BikeParking'] = 1\n",
    "\n",
    "        if row['BusinessParking_street']=='True':\n",
    "            df_business_filadelfia.at[index, 'BusinessParking_street'] = 1\n",
    "\n",
    "        if row['GoodForKids']=='True':\n",
    "            df_business_filadelfia.at[index, 'GoodForKids'] = 1\n",
    "\n",
    "    df_business_filadelfia=df_business_filadelfia.drop(['attributes', 'categories','name'],axis=1)\n",
    "\n",
    "    columns_to_scale = [col for col in df_business_filadelfia.columns if col != 'business_id']\n",
    "\n",
    "    print(columns_to_scale)\n",
    "\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    df_business_filadelfia[columns_to_scale] = scaler.fit_transform(df_business_filadelfia[columns_to_scale])\n",
    "\n",
    "    return df_business_filadelfia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_user(df_user_filadelfia):\n",
    "\n",
    "\n",
    "    yelping_since = pd.to_datetime(df_user_filadelfia['yelping_since'])\n",
    "\n",
    "    # Calcule o número de anos no Yelp\n",
    "    current_year = datetime.now().year\n",
    "    df_user_filadelfia['yelping_since'] = current_year - yelping_since.dt.year\n",
    "\n",
    "\n",
    "    df_user_filadelfia.columns = ['user_id', 'name', 'review_count_user', 'yelping_years', 'elite', 'average_stars']\n",
    "    df_user_filadelfia=df_user_filadelfia.drop(['elite','name'],axis=1)\n",
    "    # df_user_filadelfia=df_user_filadelfia.drop('yelping_since',axis=1)\n",
    "\n",
    "    columns_to_scale = [col for col in df_user_filadelfia.columns if col != 'user_id']\n",
    "\n",
    "    print(columns_to_scale)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    df_user_filadelfia[columns_to_scale] = scaler.fit_transform(df_user_filadelfia[columns_to_scale])\n",
    "\n",
    "    return df_user_filadelfia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profiling(X_train, method, n_components, users_with_5_plus_reviews, restaurants_with_5_plus_reviews):\n",
    "    \n",
    "    # this outputs the topic matrix according to the method chosen (bag-of-word, lsa, lda and doc2vec) \n",
    "    topic_matrix, n_components = feature_engineering(X_train, method, n_components)\n",
    "\n",
    "    # attach topic matrix to the dataset\n",
    "    column_names = ['comp_{}'.format(i+1) for i in range(n_components)]\n",
    "    topics = pd.DataFrame(topic_matrix, columns=column_names)\n",
    "\n",
    "    # create profiles\n",
    "    user_profile = pd.concat([X_train, topics], axis=1).drop(columns=['business_id', 'text']).groupby(\"user_id\", as_index=False)[column_names].mean()\n",
    "    restaurant_profile = pd.concat([X_train, topics], axis=1).drop(columns=['user_id', 'text']).groupby(\"business_id\", as_index=False)[column_names].mean()\n",
    "\n",
    "    # filtering for the ones with 5+ reviews (more representative)\n",
    "    user_profile = user_profile[user_profile['user_id'].isin(users_with_5_plus_reviews)].reset_index(drop=True)\n",
    "    restaurant_profile = restaurant_profile[restaurant_profile['business_id'].isin(restaurants_with_5_plus_reviews)].reset_index(drop=True)\n",
    "\n",
    "    return user_profile, restaurant_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divisão teste e treino\n",
    "#por enquanto está assim mas depois temos de definir como vamos querer dividir \n",
    "def split_data(final_data,business_data,users_data):\n",
    "\n",
    "    general_trainset, general_testset = train_test_split(final_data, test_size=0.20, random_state=42)\n",
    "\n",
    "    # Criar users_trainset e users_testset\n",
    "    users_trainset = users_data[users_data['user_id'].isin(general_trainset['user_id'])]\n",
    "    users_testset = users_data[users_data['user_id'].isin(general_testset['user_id'])]\n",
    "\n",
    "    # Criar business_trainset e business_testset\n",
    "    business_trainset = business_data[business_data['business_id'].isin(general_trainset['business_id'])]\n",
    "    business_testset = business_data[business_data['business_id'].isin(general_testset['business_id'])]\n",
    "\n",
    "    # Verificar os tamanhos dos conjuntos\n",
    "    print(\"Tamanho do trainset:\", len(general_trainset))\n",
    "    print(\"Tamanho do testset:\", len(general_testset))\n",
    "    print(\"Tamanho do users_trainset:\", len(users_trainset))\n",
    "    print(\"Tamanho do users_testset:\", len(users_testset))\n",
    "    print(\"Tamanho do business_trainset:\", len(business_trainset))\n",
    "    print(\"Tamanho do business_testset:\", len(business_testset))\n",
    "\n",
    "    return general_trainset, general_testset,users_trainset, users_testset,business_trainset, business_testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df_review_filadelfia):\n",
    "    # Desired dataset shape\n",
    "    #data = df_review_filadelfia_profiles[['user_id', 'business_id', 'text', 'stars']] # only positive reviews\n",
    "    data = df_review_filadelfia[['user_id', 'business_id', 'text', 'stars']]\n",
    "\n",
    "    #data_sample = data.sample(100000, random_state=10).reset_index(drop=True)\n",
    "\n",
    "    # train-test split\n",
    "    X_train, X_test, y_train, y_test = skl_train_test_split(data[['user_id', 'business_id', 'text']], data['stars'], test_size=0.2, random_state=1)\n",
    "    X_train.reset_index(drop=True, inplace=True)\n",
    "    X_test.reset_index(drop=True, inplace=True)\n",
    "    y_train.reset_index(drop=True, inplace=True)\n",
    "    y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_similar_restaurants(restaurant_id, philly_restaurants, similarity_matrix, n=5):\n",
    "    # Obter o índice do restaurante\n",
    "    idx = philly_restaurants.index[philly_restaurants['business_id'] == restaurant_id].tolist()[0]\n",
    "    \n",
    "    # Obter similaridade do restaurante com todos os outros\n",
    "    similars_indices = similarity_matrix[idx].argsort()[::-1]  # Do mais similar para o menos similar\n",
    "    \n",
    "    # Excluir o próprio restaurante da recomendação\n",
    "    similars_indices = similars_indices[similars_indices != idx]\n",
    "    \n",
    "    # Selecionar os n mais similares\n",
    "    similars_restaurants = philly_restaurants.iloc[similars_indices[:n]]\n",
    "    \n",
    "    return similars_restaurants[['business_id', 'name', 'categories', 'stars']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para recomendar com base nos restaurantes que o usuário já avaliou bem\n",
    "def recommend_for_user(user_id, philly_restaurants, algo, similarity_matrix, n=5):\n",
    "    # Obter os restaurantes avaliados pelo usuário\n",
    "    user_reviews = ratings[ratings['user_id'] == user_id]\n",
    "    highly_rated = user_reviews[user_reviews['stars'] >= 4]['business_id']\n",
    "    \n",
    "    # Para cada restaurante que o usuário gostou, recomendar restaurantes similares\n",
    "    recommendations = pd.DataFrame()\n",
    "    # print('highly rated ',highly_rated)\n",
    "\n",
    "    for restaurant_id in highly_rated:\n",
    "        try:\n",
    "            # Obter o índice do restaurante\n",
    "            inner_id = algo.trainset.to_inner_iid(restaurant_id)\n",
    "            \n",
    "            # Obter os restaurantes mais similares usando o modelo treinado\n",
    "            neighbors = algo.get_neighbors(inner_id, k=n)\n",
    "            # print('neighbors ',neighbors)\n",
    "            # Converter os índices internos para IDs de restaurantes\n",
    "            similar_restaurant_ids_knn = [algo.trainset.to_raw_iid(inner_id) for inner_id in neighbors]\n",
    "            \n",
    "            # Obter os detalhes dos restaurantes similares usando o modelo treinado\n",
    "            similar_restaurants_knn = philly_restaurants[philly_restaurants['business_id'].isin(similar_restaurant_ids_knn)]\n",
    "            # print('similar_restaurants_knn ',similar_restaurants_knn)\n",
    "        except ValueError:\n",
    "            # Se o restaurante não estiver no conjunto de treino, retornar um DataFrame vazio\n",
    "            similar_restaurants_knn = pd.DataFrame()\n",
    "        \n",
    "        # Obter os detalhes dos restaurantes similares usando a matriz de similaridade\n",
    "        similar_restaurants_matrix = recommend_similar_restaurants(restaurant_id, philly_restaurants, similarity_matrix, n)\n",
    "        # print('similar_restaurants_matrix ',similar_restaurants_matrix)\n",
    "        # Combinar as recomendações de ambos os métodos\n",
    "        combined_recommendations = pd.concat([similar_restaurants_knn, similar_restaurants_matrix]).drop_duplicates(subset='business_id')\n",
    "        \n",
    "        recommendations = pd.concat([recommendations, combined_recommendations])\n",
    "        # print('recommendations ',recommendations)\n",
    "    # Remover duplicatas e ordenar por popularidade (opcional: você pode melhorar o critério de ordenação)\n",
    "    recommendations = recommendations.drop_duplicates(subset='name').sort_values(by='stars', ascending=False)\n",
    "    # print('recommendations ',recommendations)\n",
    "    return recommendations['name'].head(n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_similar_users(user_id, n,similarity_matrix,user_trainset):\n",
    "    if user_id in similarity_matrix.index:\n",
    "        similar_users = similarity_matrix[user_id].sort_values(ascending=False).index[1:n+1]\n",
    "    else:\n",
    "        from sklearn.neighbors import NearestNeighbors\n",
    "        knn = NearestNeighbors(n_neighbors=n, metric='cosine')\n",
    "        knn.fit(user_trainset)\n",
    "        distances, indices = knn.kneighbors(user_trainset.loc[user_id].values.reshape(1, -1), n_neighbors=n+1)\n",
    "        similar_users = user_trainset.index[indices.flatten()][1:]\n",
    "    return similar_users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommended_restaurants(user_id, similar_users, n,user_trainset):\n",
    "    target_user_ratings = user_trainset.loc[user_id]\n",
    "    target_user_visited = target_user_ratings[target_user_ratings > 0].index\n",
    "\n",
    "    similar_users_ratings = user_trainset.loc[similar_users]\n",
    "    similar_users_ratings = similar_users_ratings.drop(columns=target_user_visited, errors='ignore')\n",
    "\n",
    "    top_rated_restaurants = similar_users_ratings.mean().sort_values(ascending=False).head(n)\n",
    "    return top_rated_restaurants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommendation system\n",
    "\n",
    "def recommend(user_id, restaurant_id, df_review_filadelfia, user_profile, restaurant_profile, type='UIBH'):\n",
    "    \n",
    "    '''\n",
    "    Esstimates rating a user gives to a restaurant\n",
    "\n",
    "    Inputs:\n",
    "    user_id - the user\n",
    "    restaurant_id - the restaurant to be rated\n",
    "    df_review_filadelfia - original df with no filtering regarding the review being positive or not\n",
    "    user_profile - df with the profiles of the users (vectors from LSA/LDA/doc2vec)\n",
    "    restaurant_profile - df with the profiles of the restaurants (vectors from LSA/LDA/doc2vec)\n",
    "    type - type of recommendations (user_item, users or items) (default = \"user_item\")\n",
    "\n",
    "    Outputs:\n",
    "    Rating\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Extracting the vector relative to the user and removing the user from the profiles\n",
    "        usr_lst = user_profile[user_profile['user_id'] == user_id].drop(columns=['user_id']).fillna(0).values\n",
    "        user_profile_function = user_profile[user_profile['user_id'] != user_id].reset_index(drop=True).fillna(0)\n",
    "        \n",
    "        # Extracting the vector relative to the restaurant and removing the restaurant from the profiles\n",
    "        bus_lst = restaurant_profile[restaurant_profile['business_id'] == restaurant_id].drop(columns=['business_id']).fillna(0).values\n",
    "        restaurant_profile_function = restaurant_profile[restaurant_profile['business_id'] != restaurant_id].reset_index(drop=True).fillna(0)\n",
    "\n",
    "\n",
    "        if type == 'UIBH':\n",
    "            # Measures the similarity between user and restaurant\n",
    "            # Rating is a linear function of the similarity\n",
    "\n",
    "            # Removing the added feature so that the vectors have the same dimensions\n",
    "            usr_lst = usr_lst[:,:-3]\n",
    "            bus_lst = bus_lst[:,:-29]\n",
    "\n",
    "            similarity_score = cosine_similarity(usr_lst, bus_lst)\n",
    "            rating = map_rating(similarity_score[0][0])\n",
    "            \n",
    "        elif type == 'UBH':\n",
    "            # Measures the similarity between the user and the other users that rated the restaurant\n",
    "            # Rating is a weighted average of the ratings given by the users (weighted by the similarity)\n",
    "\n",
    "            # Getting the other users that rated the restaurant and removing user\n",
    "            users = df_review_filadelfia[df_review_filadelfia['business_id'] == restaurant_id]['user_id']\n",
    "            users = users[users != user_id]\n",
    "\n",
    "            # Due to considering only the positive reviews for the profiles, some users don't have profile\n",
    "            # This is to remove them from the users list\n",
    "            users = users[users.isin(user_profile_function['user_id'])].unique() # and remove duplicates\n",
    "\n",
    "            # Getting the ratings given by the users and averaging if there are more than one\n",
    "            users_ratings = df_review_filadelfia[df_review_filadelfia['user_id'].isin(users)][['user_id', 'stars']]\n",
    "            users_ratings = users_ratings.groupby('user_id').mean().reset_index()\n",
    "\n",
    "            # Creating a matrix for the similar users\n",
    "            user_matrix = user_profile_function[user_profile_function['user_id'].isin(users)].drop(columns=['user_id']).values\n",
    "\n",
    "            # Similarities between user and users\n",
    "            similarity_scores = cosine_similarity(usr_lst, user_matrix)\n",
    "\n",
    "            # Transform similarities into weights\n",
    "            # This assumes that there will be other similar users.\n",
    "            # If all the other users are not similar we are giving high weights to \"not similar\" users due to this rescaling\n",
    "            similarity_scores = (similarity_scores+1)/2\n",
    "            weights = similarity_scores / np.sum(similarity_scores, axis=1)\n",
    "\n",
    "            # Computing weight-averaged rating\n",
    "            rating = np.dot(weights[0], users_ratings['stars'])\n",
    "\n",
    "        elif type == 'IBH':\n",
    "            # Measures the similarity between the restaurant and other \n",
    "            # Recommends the restaurants that are most similar to the ones the user liked before\n",
    "\n",
    "            # Getting the other restaurants the user rated and removing the restaurant\n",
    "            restaurants = df_review_filadelfia[df_review_filadelfia['user_id'] == user_id]['business_id']\n",
    "            restaurants = restaurants[restaurants != restaurant_id]\n",
    "\n",
    "            # Due to considering only the positive reviews for the profiles, some restaurants don't have profile\n",
    "            # This is to remove them from the restaurants list\n",
    "            restaurants = restaurants[restaurants.isin(restaurant_profile_function['business_id'])].unique() # and remove duplicates\n",
    "\n",
    "            # Getting the ratings given by the user and averaging if there are more than one\n",
    "            user_ratings = df_review_filadelfia[df_review_filadelfia['user_id'] == user_id][['business_id', 'stars']]\n",
    "            user_ratings = user_ratings[user_ratings['business_id'].isin(restaurants)].reset_index(drop=True)\n",
    "            user_ratings = user_ratings.groupby('business_id').mean().reset_index()\n",
    "\n",
    "            # Creating a matrix for the similar restaurants\n",
    "            restaurant_matrix = restaurant_profile_function[restaurant_profile_function['business_id'].isin(restaurants)].drop(columns=['business_id']).fillna(0).values\n",
    "\n",
    "            # Similarities between the restaurant and the other restaurants\n",
    "            similarity_scores = cosine_similarity(bus_lst, restaurant_matrix)\n",
    "\n",
    "            # Transform similarities into weights\n",
    "            # This assumes that there will be other similar restaurants.\n",
    "            # If all the other restaurants are not similar we are giving high weights to \"not similar\" restaurants due to this rescaling\n",
    "            similarity_scores = (similarity_scores+1)/2\n",
    "            weights = similarity_scores / np.sum(similarity_scores, axis=1)\n",
    "\n",
    "            # Computing weight-averaged rating\n",
    "            rating = np.dot(weights[0], user_ratings['stars'])\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Invalid type. Please choose 'UIBH', 'UBH', or 'IBH'.\")\n",
    "        \n",
    "        return rating\n",
    "    \n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(ground_truth, recommendations, k):\n",
    "\n",
    "    #top k predicted restaurants\n",
    "    top_k = recommendations[:k]\n",
    "    \n",
    "    # number of relevant items in the top-k predictions\n",
    "    relevant = sum([1 for i in top_k if i in list(ground_truth)])\n",
    "\n",
    "    return relevant / k\n",
    "\n",
    "\n",
    "def recall_at_k(ground_truth, recommendations, k):\n",
    "\n",
    "    #top k predicted restaurants\n",
    "    top_k = recommendations[:k]\n",
    "    \n",
    "    # Count the number of relevant items in the top-k predictions\n",
    "    relevant = sum([1 for i in top_k if i in list(ground_truth)])\n",
    "    \n",
    "    #number of relevant items\n",
    "    relevant_total = len(ground_truth)\n",
    "    \n",
    "    return relevant / relevant_total if relevant_total > 0 else 0\n",
    "\n",
    "def calculate_precision_recall(recommendations, ground_truth, k):\n",
    "    precision_results = []\n",
    "    recall_results = []\n",
    "\n",
    "    grouped_recommendations = recommendations.groupby('user_id')\n",
    "    grouped_ground_truth = ground_truth.groupby('user_id')\n",
    "    \n",
    "    for user_id, rec_group in grouped_recommendations:\n",
    "\n",
    "        predicted_order = rec_group['business_id'].tolist()\n",
    "        user_truth = grouped_ground_truth.get_group(user_id)['business_id']\n",
    "        user_truth_stars = grouped_ground_truth.get_group(user_id)['stars']\n",
    "        user_truth = pd.concat([user_truth, user_truth_stars], axis=1)\n",
    "        user_truth = user_truth[user_truth['stars'] > 3].reset_index(drop=True)\n",
    "        user_truth = user_truth['business_id'].tolist()\n",
    "\n",
    "        precision_k = precision_at_k(user_truth, predicted_order, k)\n",
    "        recall_k = recall_at_k(user_truth, predicted_order, k)\n",
    "        \n",
    "        precision_results.append(precision_k)\n",
    "        recall_results.append(recall_k)    \n",
    "\n",
    "    return np.mean(precision_results), np.mean(recall_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_evaluate(X_test, y_test, df_review_filadelfia, user_profile, restaurant_profile, method):\n",
    "    # predicting on the test set using the recommendations function\n",
    "    X_test['star_pred'] = X_test.apply(lambda row: recommend(row['user_id'], row['business_id'], df_review_filadelfia, user_profile, restaurant_profile, type=method), axis=1)\n",
    "    y = pd.concat([X_test['user_id'], X_test['business_id'], X_test['star_pred'], y_test], axis=1).dropna()\n",
    "\n",
    "    # Filtering the users with at least x reviews\n",
    "    user_counts = y['user_id'].value_counts()\n",
    "\n",
    "    # for metrics @3 or @5, users should have at least 10 reviews\n",
    "    users_with_10_plus_reviews = user_counts[user_counts >= 10].index\n",
    "    y_filtered = y[(y['user_id'].isin(users_with_10_plus_reviews))].reset_index(drop=True)\n",
    "\n",
    "    recommendations_3_5 = y_filtered.sort_values(by=['user_id', 'star_pred'], ascending=[True, False]).reset_index(drop=True)\n",
    "    ground_truth_3_5 = y_filtered.sort_values(by=['user_id', 'stars'], ascending=[True, False]).reset_index(drop=True)\n",
    "    # ground_truth_3_5 = ground_truth_3_5[ground_truth_3_5['stars'] > 3].reset_index(drop=True)\n",
    "\n",
    "    # for metrics @10, users should have at least 20 reviews\n",
    "    users_with_20_plus_reviews = user_counts[user_counts >= 20].index\n",
    "    y_filtered_2 = y[(y['user_id'].isin(users_with_20_plus_reviews))].reset_index(drop=True)\n",
    "\n",
    "    recommendations_10 = y_filtered_2.sort_values(by=['user_id', 'star_pred'], ascending=[True, False]).reset_index(drop=True)\n",
    "    ground_truth_10 = y_filtered_2.sort_values(by=['user_id', 'stars'], ascending=[True, False]).reset_index(drop=True)\n",
    "    # ground_truth_10 = ground_truth_10[ground_truth_10['stars'] > 3].reset_index(drop=True)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y['star_pred'], y['stars']))\n",
    "    precision_3, recall_3 = calculate_precision_recall(recommendations_3_5, ground_truth_3_5, 3)\n",
    "    precision_5, recall_5 = calculate_precision_recall(recommendations_3_5, ground_truth_3_5, 5)\n",
    "    precision_10, recall_10 = calculate_precision_recall(recommendations_10, ground_truth_10, 10)\n",
    "\n",
    "    return rmse, precision_3, recall_3, precision_5, recall_5, precision_10, recall_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(column):\n",
    "    return (column - column.min()) / (column.max() - column.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "206 >= 20 reviews e 753 >= 10 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_pre_processing = ['with lemma', 'with stemma']\n",
    "# methods_feature_engineering = ['bag of words', 'word embeddings', 'lda']\n",
    "methods_feature_engineering = ['lda', 'lsa', 'doc2vec']\n",
    "# add_features_decision = ['yes','no']\n",
    "algorithms_cf = ['CF-UB','CF-IB'] #CF-IB(Colaborative Filtering Item Based),CF-UB(Colaborative Filtering User Based)\n",
    "algorithms_content = ['UBH','IBH','UIBH'] #UBH(User Based Hybrid), IBH(Item Based Hybrid), UIBH(User Item Based Hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main J\n",
    "\n",
    "def main():\n",
    "\n",
    "    metrics = pd.DataFrame(columns=['Pre-processing', 'Feature Engineering', 'Algorithm', 'RMSE', 'Precision@3', 'Recall@3', 'Precision@5', 'Recall@5', 'Precision@10', 'Recall@10'])\n",
    "\n",
    "    # Load the dataset\n",
    "    df_business_filadelfia,df_review_filadelfia,df_user_filadelfia, users_with_5_plus_reviews, restaurants_with_5_plus_reviews = load_data()\n",
    "\n",
    "    df_review_filadelfia = df_review_filadelfia.sample(100000, random_state=10).reset_index(drop=True) # TO TEST AND DELETE AFTER\n",
    "\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_review_filadelfia)\n",
    "\n",
    "    users_train = X_train['user_id']\n",
    "\n",
    "    businesses_train = X_train['business_id']\n",
    "\n",
    "    df_user_filadelfia_treino = df_user_filadelfia[df_user_filadelfia['user_id'].isin(users_train)]\n",
    "\n",
    "    df_business_filadelfia_treino = df_business_filadelfia[df_business_filadelfia['business_id'].isin(businesses_train)]\n",
    "\n",
    "    df_features_user = features_user(df_user_filadelfia_treino)\n",
    "    df_features_business = features_business(df_business_filadelfia_treino)\n",
    "\n",
    "    # Collaborative Filtering (ACHO QUE O SPLIT NÃO VAI FUNCIONAR PARA AQUI - ACHO QUE VALE A PENA TER UMA MAIN SÓ PARA O CF)\n",
    "    # for a in algorithms_cf:\n",
    "    #     if a == 'CF-UB':\n",
    "    #         algo = KNNBasic(sim_options={'user_based': True})\n",
    "    #         algo.fit(user_trainset)\n",
    "    #         predictions = algo.test(user_testset)\n",
    "\n",
    "    #     else:\n",
    "\n",
    "    #         algo = KNNBasic(sim_options={'user_based': False})\n",
    "    #         algo.fit(business_trainset)\n",
    "    #         predictions = algo.test(business_testset)  \n",
    "\n",
    "    # Content-based / Hybrid\n",
    "    for a in methods_pre_processing:\n",
    "        X_train_pre_processed = pre_processing(X_train, a)\n",
    "\n",
    "        for b in methods_feature_engineering:\n",
    "\n",
    "            if b == 'doc2vec':\n",
    "                n_components = 100\n",
    "            else:\n",
    "                n_components = 8\n",
    "\n",
    "            user_profile, restaurant_profile = profiling(X_train_pre_processed, b, n_components, users_with_5_plus_reviews, restaurants_with_5_plus_reviews)\n",
    "\n",
    "            user_profile=user_profile.merge(df_features_user,on='user_id')\n",
    "            restaurant_profile=restaurant_profile.merge(df_features_business,on='business_id')\n",
    "\n",
    "            print(X_test)\n",
    "\n",
    "            for c in algorithms_content:\n",
    "                print(a,b,c)\n",
    "                # return(X_test, y_test, user_profile, restaurant_profile, df_review_filadelfia)\n",
    "                rmse, precision_3, recall_3, precision_5, recall_5, precision_10, recall_10 = test_evaluate(X_test, y_test, df_review_filadelfia, user_profile, restaurant_profile, c)\n",
    "                # print(f'{a} - {b} - {c} - {rmse} - {precision_3} - {recall_3} - {precision_5} - {recall_5} - {precision_10} - {recall_10}')\n",
    "                metrics = metrics.append({'Pre-processing': a, 'Feature Engineering': b, 'Algorithm': c, 'RMSE': rmse, 'Precision@3':precision_3, 'Recall@3':recall_3, 'Precision@5':precision_5, 'Recall@5':recall_5, 'Precision@10':precision_10, 'Recall@10':recall_10}, ignore_index=True)\n",
    "                metrics.to_csv('metrics.csv')\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\helen\\AppData\\Local\\Temp\\ipykernel_9692\\1876865758.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_user_filadelfia['yelping_since'] = current_year - yelping_since.dt.year\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['review_count_user', 'yelping_years', 'average_stars']\n",
      "['stars', 'review_count', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday', 'Restaurants', 'Food', 'Nightlife', 'Bars', 'Sandwiches', 'American (New)', 'Pizza', 'Breakfast & Brunch', 'American (Traditional)', 'Coffee & Tea', 'Restaurantes', 'RestaurantsTakeOut', 'BusinessAcceptsCreditCards', 'RestaurantsDelivery', 'RestaurantsAttire_casual', 'HasTV', 'RestaurantsGoodForGroups', 'BikeParking', 'BusinessParking_street', 'GoodForKids']\n",
      "                      user_id  review_count_user  yelping_years  average_stars\n",
      "4      NIhcRW6DWvk1JQhDhXwgOQ           0.130646       0.941176       0.488281\n",
      "6      AkBtT43dYcttxQ3qOzPBAg           0.058854       0.823529       0.546875\n",
      "7      RDTVzWPoCeGaUujrHIWRBQ           0.008530       0.647059       0.652344\n",
      "8      IpLRJY4CP3fXtlEd8Y4GFQ           0.029312       0.705882       0.199219\n",
      "9      RgDVC3ZUBqpEe6Y1kPhIpw           0.073052       0.705882       0.675781\n",
      "...                       ...                ...            ...            ...\n",
      "18810  xASVspzBP-HSUrTvL4qUSg           0.005038       0.176471       0.824219\n",
      "18816  tK1gqTB7G8xjcbI_NPuMNw           0.002347       0.352941       0.625000\n",
      "18833  nfP7jRgmR_wkM10_Gg0scg           0.005095       0.294118       0.515625\n",
      "18834  fwUefOXIVN4WyzMhF8_utg           0.003092       0.117647       0.570312\n",
      "18842  Rch0FkTiwah-3WMFCTPwvg           0.004065       0.470588       0.957031\n",
      "\n",
      "[7943 rows x 4 columns]\n",
      "                 business_id  stars  review_count  Monday  Tuesday  Wednesday  \\\n",
      "0     MTSW4McQd7CbVtyjqoe9mw  0.750      0.013121     1.0      1.0        1.0   \n",
      "1     MUTTqe8uqyMdBl186RmNeA  0.750      0.041987     0.0      1.0        1.0   \n",
      "2     ROeacJQwBeh05Rqg7F6TCg  0.875      0.034990     1.0      1.0        1.0   \n",
      "3     aPNXGTDkf-4bjhyMBQxqpQ  0.625      0.010497     0.2      0.0        1.0   \n",
      "4     ppFCk9aQkM338Rgwpl2F5A  0.500      0.008922     0.2      0.2        0.2   \n",
      "...                      ...    ...           ...     ...      ...        ...   \n",
      "2811                     NaN    NaN           NaN     NaN      NaN        NaN   \n",
      "2824                     NaN    NaN           NaN     NaN      NaN        NaN   \n",
      "2865                     NaN    NaN           NaN     NaN      NaN        NaN   \n",
      "2882                     NaN    NaN           NaN     NaN      NaN        NaN   \n",
      "2894                     NaN    NaN           NaN     NaN      NaN        NaN   \n",
      "\n",
      "      Thursday  Friday  Saturday  Sunday  ...  Restaurantes  \\\n",
      "0          1.0     1.0       1.0     1.0  ...           0.0   \n",
      "1          1.0     1.0       1.0     1.0  ...           0.0   \n",
      "2          1.0     1.0       1.0     0.0  ...           0.0   \n",
      "3          1.0     1.0       1.0     1.0  ...           0.0   \n",
      "4          0.2     0.2       0.2     0.2  ...           0.0   \n",
      "...        ...     ...       ...     ...  ...           ...   \n",
      "2811       NaN     NaN       NaN     NaN  ...           NaN   \n",
      "2824       NaN     NaN       NaN     NaN  ...           NaN   \n",
      "2865       NaN     NaN       NaN     NaN  ...           NaN   \n",
      "2882       NaN     NaN       NaN     NaN  ...           NaN   \n",
      "2894       NaN     NaN       NaN     NaN  ...           NaN   \n",
      "\n",
      "      RestaurantsTakeOut  BusinessAcceptsCreditCards  RestaurantsDelivery  \\\n",
      "0                    1.0                         0.0                  0.0   \n",
      "1                    1.0                         1.0                  1.0   \n",
      "2                    1.0                         1.0                  0.0   \n",
      "3                    1.0                         1.0                  1.0   \n",
      "4                    1.0                         1.0                  1.0   \n",
      "...                  ...                         ...                  ...   \n",
      "2811                 1.0                         1.0                  1.0   \n",
      "2824                 1.0                         1.0                  NaN   \n",
      "2865                 1.0                         NaN                  1.0   \n",
      "2882                 1.0                         1.0                  1.0   \n",
      "2894                 1.0                         1.0                  1.0   \n",
      "\n",
      "      RestaurantsAttire_casual  HasTV  RestaurantsGoodForGroups  BikeParking  \\\n",
      "0                          0.0    0.0                       0.0          1.0   \n",
      "1                          1.0    0.0                       1.0          0.0   \n",
      "2                          1.0    1.0                       0.0          1.0   \n",
      "3                          0.0    1.0                       0.0          0.0   \n",
      "4                          1.0    1.0                       1.0          1.0   \n",
      "...                        ...    ...                       ...          ...   \n",
      "2811                       1.0    1.0                       1.0          NaN   \n",
      "2824                       1.0    NaN                       1.0          NaN   \n",
      "2865                       NaN    NaN                       NaN          NaN   \n",
      "2882                       NaN    1.0                       NaN          NaN   \n",
      "2894                       NaN    1.0                       NaN          1.0   \n",
      "\n",
      "      BusinessParking_street  GoodForKids  \n",
      "0                        0.0          0.0  \n",
      "1                        0.0          0.0  \n",
      "2                        0.0          1.0  \n",
      "3                        0.0          1.0  \n",
      "4                        0.0          1.0  \n",
      "...                      ...          ...  \n",
      "2811                     NaN          1.0  \n",
      "2824                     NaN          1.0  \n",
      "2865                     NaN          NaN  \n",
      "2882                     NaN          NaN  \n",
      "2894                     NaN          NaN  \n",
      "\n",
      "[3021 rows x 30 columns]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m metrics_final \u001b[38;5;241m=\u001b[39m main()\n\u001b[0;32m      2\u001b[0m metrics_final\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics_final.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[25], line 55\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     n_components \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m---> 55\u001b[0m user_profile, restaurant_profile \u001b[38;5;241m=\u001b[39m profiling(X_train_pre_processed, b, n_components, users_with_5_plus_reviews, restaurants_with_5_plus_reviews)\n\u001b[0;32m     57\u001b[0m user_profile\u001b[38;5;241m=\u001b[39muser_profile\u001b[38;5;241m.\u001b[39mmerge(df_features_user,on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     58\u001b[0m restaurant_profile\u001b[38;5;241m=\u001b[39mrestaurant_profile\u001b[38;5;241m.\u001b[39mmerge(df_features_business,on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbusiness_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m, in \u001b[0;36mprofiling\u001b[1;34m(X_train, method, n_components, users_with_5_plus_reviews, restaurants_with_5_plus_reviews)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprofiling\u001b[39m(X_train, method, n_components, users_with_5_plus_reviews, restaurants_with_5_plus_reviews):\n\u001b[0;32m      2\u001b[0m     \n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# this outputs the topic matrix according to the method chosen (bag-of-word, lsa, lda and doc2vec) \u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     topic_matrix, n_components \u001b[38;5;241m=\u001b[39m feature_engineering(X_train, method, n_components)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# attach topic matrix to the dataset\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     column_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomp_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_components)]\n",
      "Cell \u001b[1;32mIn[9], line 15\u001b[0m, in \u001b[0;36mfeature_engineering\u001b[1;34m(data_reviews, method, components)\u001b[0m\n\u001b[0;32m     13\u001b[0m     count_matrix \u001b[38;5;241m=\u001b[39m count_vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(data_reviews[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     14\u001b[0m     lda_model \u001b[38;5;241m=\u001b[39m LatentDirichletAllocation(n_components\u001b[38;5;241m=\u001b[39mcomponents, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m     matrix \u001b[38;5;241m=\u001b[39m lda_model\u001b[38;5;241m.\u001b[39mfit_transform(count_matrix)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# df = pd.DataFrame(matrix, columns=[f'Topic_{i}' for i in range(lda_model.n_components)])\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlsa\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:915\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:673\u001b[0m, in \u001b[0;36mLatentDirichletAllocation.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    665\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_em_step(\n\u001b[0;32m    666\u001b[0m             X[idx_slice, :],\n\u001b[0;32m    667\u001b[0m             total_samples\u001b[38;5;241m=\u001b[39mn_samples,\n\u001b[0;32m    668\u001b[0m             batch_update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    669\u001b[0m             parallel\u001b[38;5;241m=\u001b[39mparallel,\n\u001b[0;32m    670\u001b[0m         )\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    672\u001b[0m     \u001b[38;5;66;03m# batch update\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_em_step(\n\u001b[0;32m    674\u001b[0m         X, total_samples\u001b[38;5;241m=\u001b[39mn_samples, batch_update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, parallel\u001b[38;5;241m=\u001b[39mparallel\n\u001b[0;32m    675\u001b[0m     )\n\u001b[0;32m    677\u001b[0m \u001b[38;5;66;03m# check perplexity\u001b[39;00m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_every \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m evaluate_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:524\u001b[0m, in \u001b[0;36mLatentDirichletAllocation._em_step\u001b[1;34m(self, X, total_samples, batch_update, parallel)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"EM update for 1 iteration.\u001b[39;00m\n\u001b[0;32m    498\u001b[0m \n\u001b[0;32m    499\u001b[0m \u001b[38;5;124;03mupdate `_component` by batch VB or online VB.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;124;03m    Unnormalized document topic distribution.\u001b[39;00m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;66;03m# E-step\u001b[39;00m\n\u001b[1;32m--> 524\u001b[0m _, suff_stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_e_step(\n\u001b[0;32m    525\u001b[0m     X, cal_sstats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, parallel\u001b[38;5;241m=\u001b[39mparallel\n\u001b[0;32m    526\u001b[0m )\n\u001b[0;32m    528\u001b[0m \u001b[38;5;66;03m# M-step\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_update:\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:467\u001b[0m, in \u001b[0;36mLatentDirichletAllocation._e_step\u001b[1;34m(self, X, cal_sstats, random_init, parallel)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parallel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    466\u001b[0m     parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 467\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    468\u001b[0m     delayed(_update_doc_distribution)(\n\u001b[0;32m    469\u001b[0m         X[idx_slice, :],\n\u001b[0;32m    470\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexp_dirichlet_component_,\n\u001b[0;32m    471\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc_topic_prior_,\n\u001b[0;32m    472\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_doc_update_iter,\n\u001b[0;32m    473\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_change_tol,\n\u001b[0;32m    474\u001b[0m         cal_sstats,\n\u001b[0;32m    475\u001b[0m         random_state,\n\u001b[0;32m    476\u001b[0m     )\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx_slice \u001b[38;5;129;01min\u001b[39;00m gen_even_slices(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], n_jobs)\n\u001b[0;32m    478\u001b[0m )\n\u001b[0;32m    480\u001b[0m \u001b[38;5;66;03m# merge result\u001b[39;00m\n\u001b[0;32m    481\u001b[0m doc_topics, sstats_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresults)\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:144\u001b[0m, in \u001b[0;36m_update_doc_distribution\u001b[1;34m(X, exp_topic_word_distr, doc_topic_prior, max_doc_update_iter, mean_change_tol, cal_sstats, random_state)\u001b[0m\n\u001b[0;32m    140\u001b[0m last_d \u001b[38;5;241m=\u001b[39m doc_topic_d\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# The optimal phi_{dwk} is proportional to\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# exp(E[log(theta_{dk})]) * exp(E[log(beta_{dw})]).\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m norm_phi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(exp_doc_topic_d, exp_topic_word_d) \u001b[38;5;241m+\u001b[39m eps\n\u001b[0;32m    146\u001b[0m doc_topic_d \u001b[38;5;241m=\u001b[39m exp_doc_topic_d \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(cnts \u001b[38;5;241m/\u001b[39m norm_phi, exp_topic_word_d\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# Note: adds doc_topic_prior to doc_topic_d, in-place.\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "metrics_final = main()\n",
    "metrics_final.to_csv('metrics_final.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mecd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
