{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\helen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\helen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\helen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import ast\n",
    "from surprise import Dataset, Reader, KNNBasic\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from surprise.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split as skl_train_test_split\n",
    "from datetime import datetime\n",
    "from gensim.models.doc2vec import Doc2Vec,TaggedDocument\n",
    "\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "\n",
    "df_business = pd.read_csv('yelp_academic_dataset_business.csv')\n",
    "df_checkin = pd.read_csv('yelp_academic_dataset_checkin.csv')\n",
    "df_review=pd.read_csv('yelp_academic_dataset_review.csv')\n",
    "df_tip=pd.read_csv('yelp_academic_dataset_tip.csv')\n",
    "df_user=pd.read_csv('yelp_academic_dataset_user.csv')\n",
    "\n",
    "# print(df_business.columns)\n",
    "# print(df_checkin.columns)\n",
    "# print(df_review.columns)\n",
    "# print(df_tip.columns)\n",
    "# print(df_user.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_rating(cosine_similarity):\n",
    "    '''\n",
    "    Maps a consine similarity score to a rating from 1 to 5\n",
    "    '''\n",
    "    return 1 + 4 * ((cosine_similarity + 1) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "\n",
    "def load_data():\n",
    "    # df_business = pd.read_csv('yelp_academic_dataset_business.csv')\n",
    "    # df_review=pd.read_csv('yelp_academic_dataset_review.csv')\n",
    "    # df_user=pd.read_csv('yelp_academic_dataset_user.csv')\n",
    "    \n",
    "    df_business_filadelfia = df_business[(df_business['city'] == 'Philadelphia') & (df_business['categories'].str.contains('Restaurants', na=False)) & (df_business['is_open']==1)].reset_index(drop=True)\n",
    "    df_business_filadelfia=df_business_filadelfia[['business_id', 'name', 'stars', 'review_count', 'attributes', 'categories', 'hours']].reset_index(drop=True)\n",
    "    df_business_filadelfia = df_business_filadelfia.dropna().reset_index(drop=True)\n",
    "\n",
    "    df_review_filadelfia = df_review[df_review['business_id'].isin(df_business_filadelfia['business_id'])]\n",
    "    df_review_filadelfia=df_review_filadelfia[['review_id', 'user_id', 'business_id', 'stars', 'text', 'date']]\n",
    "    df_review_filadelfia['liked'] = (df_review_filadelfia['stars'] > 3).astype(int)\n",
    "    df_review_filadelfia_profiles = df_review_filadelfia[df_review_filadelfia['liked'] == 1].reset_index(drop=True)\n",
    "    df_review_filadelfia = df_review_filadelfia.dropna().reset_index(drop=True)\n",
    "\n",
    "    df_user_filadelfia = df_user[df_user['user_id'].isin(df_review_filadelfia['user_id'])]\n",
    "    df_user_filadelfia=df_user_filadelfia[['user_id', 'name', 'review_count', 'yelping_since', 'elite', 'average_stars']].reset_index(drop=True)\n",
    "    df_user_filadelfia = df_user_filadelfia.dropna().reset_index(drop=True)\n",
    "\n",
    "    # counts\n",
    "    user_counts = df_review_filadelfia['user_id'].value_counts()\n",
    "    restaurant_counts = df_review_filadelfia['business_id'].value_counts()\n",
    "\n",
    "    # creating filters for users and restaurants with 5+ reviews\n",
    "    users_with_5_plus_reviews = user_counts[user_counts >= 5].index\n",
    "    restaurants_with_5_plus_reviews = restaurant_counts[restaurant_counts >= 5].index\n",
    "\n",
    "    return df_business_filadelfia,df_review_filadelfia,df_user_filadelfia, users_with_5_plus_reviews, restaurants_with_5_plus_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    words = text.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def stem_text(text):\n",
    "    words = text.split()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(stemmed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processamento\n",
    "def pre_processing(data_review,method):\n",
    "    if method == 'with lemma':\n",
    "\n",
    "        data_review['text'] = data_review['text'].apply(lemmatize_text)\n",
    "\n",
    "    \n",
    "    else: \n",
    "        data_review['text'] = data_review['text'].apply(stem_text)\n",
    "\n",
    "    return data_review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering\n",
    "def feature_engineering(data_reviews, method, components=8):\n",
    "    if method == 'bag of words':\n",
    "        vectorizer = CountVectorizer(max_df=0.95, min_df=2)\n",
    "        matrix = vectorizer.fit_transform(data_reviews['text'])\n",
    "        matrix = matrix.toarray()\n",
    "        components = matrix.shape[1]\n",
    "        # feature_names = vectorizer.get_feature_names_out()\n",
    "        # df = pd.DataFrame(matrix, columns=feature_names)\n",
    "   \n",
    "    elif method =='lda':\n",
    "        count_vectorizer = CountVectorizer(max_df=0.95, min_df=2)\n",
    "        count_matrix = count_vectorizer.fit_transform(data_reviews['text'])\n",
    "        lda_model = LatentDirichletAllocation(n_components=components, random_state=42)\n",
    "        matrix = lda_model.fit_transform(count_matrix)\n",
    "        # df = pd.DataFrame(matrix, columns=[f'Topic_{i}' for i in range(lda_model.n_components)])\n",
    "    \n",
    "    elif method =='lsa':\n",
    "        vectorizer = CountVectorizer()\n",
    "        count_matrix = vectorizer.fit_transform(data_reviews['text'])\n",
    "        lsa_model = TruncatedSVD(n_components=components)\n",
    "        matrix = lsa_model.fit_transform(count_matrix)\n",
    "\n",
    "    elif method == 'doc2vec':\n",
    "        # preproces the documents, and create TaggedDocuments\n",
    "        tagged_data = [TaggedDocument(words=word_tokenize(doc.lower()),\n",
    "                                    tags=[str(i)]) for i,\n",
    "                    doc in enumerate(data_reviews['text'])]\n",
    "\n",
    "        # Doc2vec model\n",
    "        model = Doc2Vec(vector_size=components,\n",
    "                        min_count=2, epochs=50)\n",
    "        model.build_vocab(tagged_data)\n",
    "        model.train(tagged_data,\n",
    "                    total_examples=model.corpus_count,\n",
    "                    epochs=model.epochs)\n",
    "\n",
    "        # document vectors\n",
    "        matrix = [model.infer_vector(\n",
    "            word_tokenize(doc.lower())) for doc in data_reviews['text']]\n",
    "\n",
    "    return matrix, components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_business(df_business_filadelfia):\n",
    "    df_business_filadelfia = df_business_filadelfia[['business_id', 'name', 'stars', 'review_count','attributes', 'categories', 'hours']]\n",
    "\n",
    "\n",
    "    #variavel horario\n",
    "    df_business_filadelfia['hours'] = df_business_filadelfia['hours'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else {})\n",
    "\n",
    "    # Crie colunas separadas para cada dia da semana\n",
    "    dias_da_semana = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "    for dia in dias_da_semana:\n",
    "        df_business_filadelfia[dia] = df_business_filadelfia['hours'].apply(lambda x: x.get(dia, None))\n",
    "\n",
    "    # Remova a coluna original 'hours' se não for mais necessária\n",
    "    df_business_filadelfia.drop(columns=['hours'], inplace=True)\n",
    "\n",
    "\n",
    "    def categorize_hours(hours):\n",
    "\n",
    "        if hours == None:\n",
    "            return 0 #'Fechado'\n",
    "        \n",
    "        else:\n",
    "            start_time, end_time = hours.split('-')\n",
    "            start_hour = int(start_time.split(':')[0])\n",
    "            end_hour = int(end_time.split(':')[0])\n",
    "            \n",
    "            if end_hour <= 12:\n",
    "                return 1 #'Manhã'\n",
    "            elif 12 < start_hour and end_hour<=15:\n",
    "                return 2 #'Almoço'\n",
    "            elif start_hour > 15 and end_hour < 19:\n",
    "                return 3 #'Tarde'\n",
    "            elif start_hour>=19:\n",
    "                return 4 #'Noite'\n",
    "            else:\n",
    "                return 5 #'Dia todo'\n",
    "\n",
    "    # Aplicar a função de categorização a cada coluna de dia da semana\n",
    "    for day in ['Monday', 'Tuesday', 'Wednesday','Thursday', 'Friday', 'Saturday', 'Sunday']:\n",
    "        df_business_filadelfia[day] = df_business_filadelfia[day].apply(categorize_hours)\n",
    "\n",
    "\n",
    "    #variavel categoria\n",
    "\n",
    "    df_business_filadelfia['Restaurants'] = 0\n",
    "    df_business_filadelfia['Food'] = 0\n",
    "    df_business_filadelfia['Nightlife'] = 0\n",
    "    df_business_filadelfia['Bars'] = 0\n",
    "    df_business_filadelfia['Sandwiches'] = 0\n",
    "    df_business_filadelfia['American (New)'] = 0\n",
    "    df_business_filadelfia['Pizza'] = 0\n",
    "    df_business_filadelfia['Breakfast & Brunch'] = 0\n",
    "    df_business_filadelfia['American (Traditional)'] = 0\n",
    "    df_business_filadelfia['Coffee & Tea'] = 0\n",
    "\n",
    "    for index, row in df_business_filadelfia.iterrows():\n",
    "        categories = row['categories']\n",
    "        if 'Restaurants' in categories:\n",
    "            df_business_filadelfia.at[index, 'Restaurantes'] = 1\n",
    "\n",
    "        if 'Food' in categories:\n",
    "            df_business_filadelfia.at[index, 'Food'] = 1\n",
    "\n",
    "        if 'Nightlife' in categories:\n",
    "            df_business_filadelfia.at[index, 'Nightlife'] = 1\n",
    "\n",
    "        if 'Bars' in categories:\n",
    "            df_business_filadelfia.at[index, 'Bars'] = 1\n",
    "\n",
    "        if 'Sandwiches' in categories:\n",
    "            df_business_filadelfia.at[index, 'Sandwiches'] = 1\n",
    "\n",
    "        if 'American (New)' in categories:\n",
    "            df_business_filadelfia.at[index, 'American (New)'] = 1\n",
    "        \n",
    "        if 'Pizza' in categories:\n",
    "            df_business_filadelfia.at[index, 'Pizza'] = 1\n",
    "\n",
    "        if 'Breakfast & Brunch' in categories:\n",
    "            df_business_filadelfia.at[index, 'Breakfast & Brunch'] = 1\n",
    "\n",
    "        if 'American (Traditional)' in categories:\n",
    "            df_business_filadelfia.at[index, 'American (Traditional)'] = 1\n",
    "\n",
    "        if 'Coffee & Tea' in categories:\n",
    "            df_business_filadelfia.at[index, 'Coffee & Tea'] = 1\n",
    "\n",
    "\n",
    "\n",
    "    #variavel atributos\n",
    "\n",
    "    df_business_filadelfia['RestaurantsTakeOut'] = 0\n",
    "    df_business_filadelfia['BusinessAcceptsCreditCards'] = 0\n",
    "    df_business_filadelfia['RestaurantsDelivery'] = 0\n",
    "    df_business_filadelfia['RestaurantsAttire_casual'] = 0\n",
    "    df_business_filadelfia['HasTV'] = 0\n",
    "    df_business_filadelfia['RestaurantsGoodForGroups'] = 0\n",
    "    df_business_filadelfia['BikeParking'] = 0\n",
    "    df_business_filadelfia['BusinessParking_street'] = 0\n",
    "    df_business_filadelfia['GoodForKids'] = 0\n",
    "\n",
    "\n",
    "    df_business_filadelfia['attributes'] = df_business_filadelfia['attributes'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else {})\n",
    "\n",
    "    df_atributos = pd.json_normalize(df_business_filadelfia['attributes'])\n",
    "    # Normalizar a coluna 'BusinessParking' para expandir as sub-chaves\n",
    "    if 'BusinessParking' in df_atributos:\n",
    "        df_parking = pd.json_normalize(df_atributos['BusinessParking'].dropna().apply(ast.literal_eval), sep='_')\n",
    "        df_parking.columns = ['BusinessParking_garage', 'BusinessParking_street', 'BusinessParking_validated', 'BusinessParking_lot', 'BusinessParking_valet']\n",
    "        # Juntar o df_parking ao df_atributos\n",
    "        df_atributos = df_atributos.drop(columns=['BusinessParking']).join(df_parking, how='left')\n",
    "\n",
    "    if \"Ambience\" in df_atributos:\n",
    "        df_ambience = pd.json_normalize(df_atributos['Ambience'].dropna().apply(ast.literal_eval), sep='_')\n",
    "        df_ambience.columns = [f\"Ambience_{col}\" for col in df_ambience.columns]\n",
    "        # Juntar o df_ambience ao df_atributos\n",
    "        df_atributos = df_atributos.drop(columns=['Ambience']).join(df_ambience, how='left')\n",
    "\n",
    "    if \"GoodForMeal\" in df_atributos:\n",
    "        df_GoodForMeal = pd.json_normalize(df_atributos['GoodForMeal'].dropna().apply(ast.literal_eval), sep='_')\n",
    "        df_GoodForMeal.columns = [f\"GoodForMeal_{col}\" for col in df_GoodForMeal.columns]\n",
    "        # Juntar o df_GoodForMeal ao df_atributos\n",
    "        df_atributos = df_atributos.drop(columns=['GoodForMeal']).join(df_GoodForMeal, how='left')\n",
    "\n",
    "    if \"Music\" in df_atributos:\n",
    "        df_Music = pd.json_normalize(df_atributos['Music'].dropna().apply(ast.literal_eval), sep='_')\n",
    "        df_Music.columns = [f\"Music_{col}\" for col in df_Music.columns]\n",
    "        # Juntar o df_Music ao df_atributos\n",
    "        df_atributos = df_atributos.drop(columns=['Music']).join(df_Music, how='left')\n",
    "\n",
    "    if \"BestNights\" in df_atributos:\n",
    "        df_BestNights = pd.json_normalize(df_atributos['BestNights'].dropna().apply(ast.literal_eval), sep='_')\n",
    "        df_BestNights.columns = [f\"BestNights_{col}\" for col in df_BestNights.columns]\n",
    "        # Juntar o df_BestNights ao df_atributos\n",
    "        df_atributos = df_atributos.drop(columns=['BestNights']).join(df_BestNights, how='left')\n",
    "\n",
    "    if \"DietaryRestrictions\" in df_atributos:\n",
    "        df_DietaryRestrictions = pd.json_normalize(df_atributos['DietaryRestrictions'].dropna().apply(ast.literal_eval), sep='_')\n",
    "        df_DietaryRestrictions.columns = [f\"DietaryRestrictions_{col}\" for col in df_DietaryRestrictions.columns]\n",
    "        # Juntar o df_DietaryRestrictions ao df_atributos\n",
    "        df_atributos = df_atributos.drop(columns=['DietaryRestrictions']).join(df_DietaryRestrictions, how='left')\n",
    "\n",
    "    \n",
    "    df_atributos = df_atributos.applymap(lambda x: x.replace(\"u'\", \"\").replace(\"'\", \"\") if isinstance(x, str) else x)\n",
    "    def create_price_range_columns(row):\n",
    "        price_ranges = ['1', '2', '3', '4']\n",
    "        for price in price_ranges:\n",
    "            column_name = f'RestaurantsPriceRange2_{price}'\n",
    "            if row['RestaurantsPriceRange2'] == price:\n",
    "                row[column_name] = 'True'\n",
    "            elif row['RestaurantsPriceRange2'] == 'False':\n",
    "                row[column_name] = 'False'\n",
    "            else:\n",
    "                row[column_name] = 'False'\n",
    "        return row\n",
    "\n",
    "    # Aplicar a função linha por linha\n",
    "    df_atributos = df_atributos.apply(create_price_range_columns, axis=1)\n",
    "    df_atributos=df_atributos.drop('RestaurantsPriceRange2',axis=1)\n",
    "\n",
    "    def alchool_columns(row):\n",
    "        types = ['full_bar','beer_and_wine']\n",
    "        for t in types:\n",
    "            column_name = f'Alcohol_{t}'\n",
    "            if row['Alcohol'] == t:\n",
    "                row[column_name] = 'True'\n",
    "            elif row['Alcohol'] == 'False':\n",
    "                row[column_name] = 'False'\n",
    "            else:\n",
    "                row[column_name] = 'False'\n",
    "        return row\n",
    "\n",
    "    df_atributos = df_atributos.apply(alchool_columns, axis=1)\n",
    "    df_atributos=df_atributos.drop('Alcohol',axis=1)\n",
    "\n",
    "\n",
    "    def wifi_columns(row):\n",
    "        types = ['free','paid']\n",
    "        for t in types:\n",
    "            column_name = f'WiFi_{t}'\n",
    "            if row['WiFi'] == t:\n",
    "                row[column_name] = 'True'\n",
    "            elif row['WiFi'] == 'False':\n",
    "                row[column_name] = 'False'\n",
    "            else:\n",
    "                row[column_name] = 'False'\n",
    "        return row\n",
    "\n",
    "    df_atributos = df_atributos.apply(wifi_columns, axis=1)\n",
    "    df_atributos=df_atributos.drop('WiFi',axis=1)\n",
    "\n",
    "\n",
    "    def attire_columns(row):\n",
    "        types = ['casual', 'dressy', 'formal']\n",
    "        for t in types:\n",
    "            column_name = f'RestaurantsAttire_{t}'\n",
    "            if row['RestaurantsAttire'] == t:\n",
    "                row[column_name] = 'True'\n",
    "            elif row['RestaurantsAttire'] == 'False':\n",
    "                row[column_name] = 'False'\n",
    "            else:\n",
    "                row[column_name] = 'False'\n",
    "        return row\n",
    "\n",
    "    df_atributos = df_atributos.apply(attire_columns, axis=1)\n",
    "    df_atributos=df_atributos.drop('RestaurantsAttire',axis=1)\n",
    "\n",
    "    def noise_columns(row):\n",
    "        types = ['average', 'quiet', 'loud', 'very_loud']\n",
    "        for t in types:\n",
    "            column_name = f'NoiseLevel_{t}'\n",
    "            if row['NoiseLevel'] == t:\n",
    "                row[column_name] = 'True'\n",
    "            elif row['NoiseLevel'] == 'False':\n",
    "                row[column_name] = 'False'\n",
    "            else:\n",
    "                row[column_name] = 'False'\n",
    "        return row\n",
    "\n",
    "    df_atributos = df_atributos.apply(noise_columns, axis=1)\n",
    "    df_atributos=df_atributos.drop('NoiseLevel',axis=1)\n",
    "\n",
    "    def ages_columns(row):\n",
    "        types = ['21plus', 'allages']\n",
    "        for t in types:\n",
    "            column_name = f'AgesAllowed_{t}'\n",
    "            if row['AgesAllowed'] == t:\n",
    "                row[column_name] = 'True'\n",
    "            elif row['AgesAllowed'] == 'False':\n",
    "                row[column_name] = 'False'\n",
    "            else:\n",
    "                row[column_name] = 'False'\n",
    "        return row\n",
    "\n",
    "    df_atributos = df_atributos.apply(ages_columns, axis=1)\n",
    "    df_atributos=df_atributos.drop('AgesAllowed',axis=1)\n",
    "\n",
    "    df_atributos = df_atributos.replace('None','False')\n",
    "    df_atributos = df_atributos.replace( np.nan,'False')\n",
    "    df_atributos = df_atributos.replace('none','False')\n",
    "    df_atributos = df_atributos.replace('no','False')\n",
    "    df_atributos = df_atributos.replace('yes','True')\n",
    "    df_atributos['Smoking'] = df_atributos['Smoking'].replace('outdoor','True')\n",
    "    df_atributos['BYOBCorkage'] = df_atributos['BYOBCorkage'].replace('yes_free','True')\n",
    "    df_atributos['BYOBCorkage'] = df_atributos['BYOBCorkage'].replace('yes_corkage','True')\n",
    "\n",
    "    df_atributos['business_id'] = df_business_filadelfia['business_id']\n",
    "\n",
    "\n",
    "    for index, row in df_atributos.iterrows():\n",
    "        # attributes = row['attributes']\n",
    "        if row['RestaurantsTakeOut']=='True':\n",
    "            df_business_filadelfia.at[index, 'RestaurantsTakeOut'] = 1\n",
    "\n",
    "        if row['BusinessAcceptsCreditCards']=='True':\n",
    "            df_business_filadelfia.at[index, 'BusinessAcceptsCreditCards'] = 1\n",
    "\n",
    "        if row['RestaurantsDelivery']=='True':\n",
    "            df_business_filadelfia.at[index, 'RestaurantsDelivery'] = 1\n",
    "\n",
    "        if row['RestaurantsAttire_casual']=='True':\n",
    "            df_business_filadelfia.at[index, 'RestaurantsAttire_casual'] = 1\n",
    "\n",
    "        if row['HasTV']=='True':\n",
    "            df_business_filadelfia.at[index, 'HasTV'] = 1\n",
    "\n",
    "        if row['RestaurantsGoodForGroups']=='True':\n",
    "            df_business_filadelfia.at[index, 'RestaurantsGoodForGroups'] = 1\n",
    "\n",
    "        if row['BikeParking']=='True':\n",
    "            df_business_filadelfia.at[index, 'BikeParking'] = 1\n",
    "\n",
    "        if row['BusinessParking_street']=='True':\n",
    "            df_business_filadelfia.at[index, 'BusinessParking_street'] = 1\n",
    "\n",
    "        if row['GoodForKids']=='True':\n",
    "            df_business_filadelfia.at[index, 'GoodForKids'] = 1\n",
    "\n",
    "    df_business_filadelfia=df_business_filadelfia.drop(['attributes', 'categories','name'],axis=1)\n",
    "\n",
    "    return df_business_filadelfia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_user(df_user_filadelfia):\n",
    "\n",
    "\n",
    "    yelping_since = pd.to_datetime(df_user_filadelfia['yelping_since'])\n",
    "\n",
    "    # Calcule o número de anos no Yelp\n",
    "    current_year = datetime.now().year\n",
    "    df_user_filadelfia['yelping_since'] = current_year - yelping_since.dt.year\n",
    "\n",
    "\n",
    "    df_user_filadelfia.columns = ['user_id', 'name', 'review_count_user', 'yelping_years', 'elite', 'average_stars']\n",
    "    df_user_filadelfia=df_user_filadelfia.drop(['elite','name'],axis=1)\n",
    "    # df_user_filadelfia=df_user_filadelfia.drop('yelping_since',axis=1)\n",
    "\n",
    "        \n",
    "\n",
    "    return df_user_filadelfia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profiling(X_train, method, n_components, users_with_5_plus_reviews, restaurants_with_5_plus_reviews):\n",
    "    \n",
    "    # this outputs the topic matrix according to the method chosen (bag-of-word, lsa, lda and doc2vec) \n",
    "    topic_matrix, n_components = feature_engineering(X_train, method, n_components)\n",
    "\n",
    "    # attach topic matrix to the dataset\n",
    "    column_names = ['comp_{}'.format(i+1) for i in range(n_components)]\n",
    "    topics = pd.DataFrame(topic_matrix, columns=column_names)\n",
    "\n",
    "    # create profiles\n",
    "    user_profile = pd.concat([X_train, topics], axis=1).drop(columns=['business_id', 'text']).groupby(\"user_id\", as_index=False)[column_names].mean()\n",
    "    restaurant_profile = pd.concat([X_train, topics], axis=1).drop(columns=['user_id', 'text']).groupby(\"business_id\", as_index=False)[column_names].mean()\n",
    "\n",
    "    # filtering for the ones with 5+ reviews (more representative)\n",
    "    user_profile = user_profile[user_profile['user_id'].isin(users_with_5_plus_reviews)].reset_index(drop=True)\n",
    "    restaurant_profile = restaurant_profile[restaurant_profile['business_id'].isin(restaurants_with_5_plus_reviews)].reset_index(drop=True)\n",
    "\n",
    "    return user_profile, restaurant_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divisão teste e treino\n",
    "#por enquanto está assim mas depois temos de definir como vamos querer dividir \n",
    "def split_data(final_data,business_data,users_data):\n",
    "\n",
    "    general_trainset, general_testset = train_test_split(final_data, test_size=0.20, random_state=42)\n",
    "\n",
    "    # Criar users_trainset e users_testset\n",
    "    users_trainset = users_data[users_data['user_id'].isin(general_trainset['user_id'])]\n",
    "    users_testset = users_data[users_data['user_id'].isin(general_testset['user_id'])]\n",
    "\n",
    "    # Criar business_trainset e business_testset\n",
    "    business_trainset = business_data[business_data['business_id'].isin(general_trainset['business_id'])]\n",
    "    business_testset = business_data[business_data['business_id'].isin(general_testset['business_id'])]\n",
    "\n",
    "    # Verificar os tamanhos dos conjuntos\n",
    "    print(\"Tamanho do trainset:\", len(general_trainset))\n",
    "    print(\"Tamanho do testset:\", len(general_testset))\n",
    "    print(\"Tamanho do users_trainset:\", len(users_trainset))\n",
    "    print(\"Tamanho do users_testset:\", len(users_testset))\n",
    "    print(\"Tamanho do business_trainset:\", len(business_trainset))\n",
    "    print(\"Tamanho do business_testset:\", len(business_testset))\n",
    "\n",
    "    return general_trainset, general_testset,users_trainset, users_testset,business_trainset, business_testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df_review_filadelfia):\n",
    "    # Desired dataset shape\n",
    "    #data = df_review_filadelfia_profiles[['user_id', 'business_id', 'text', 'stars']] # only positive reviews\n",
    "    data = df_review_filadelfia[['user_id', 'business_id', 'text', 'stars']]\n",
    "\n",
    "    #data_sample = data.sample(100000, random_state=10).reset_index(drop=True)\n",
    "\n",
    "    # train-test split\n",
    "    X_train, X_test, y_train, y_test = skl_train_test_split(data[['user_id', 'business_id', 'text']], data['stars'], test_size=0.2, random_state=1)\n",
    "    X_train.reset_index(drop=True, inplace=True)\n",
    "    X_test.reset_index(drop=True, inplace=True)\n",
    "    y_train.reset_index(drop=True, inplace=True)\n",
    "    y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_similar_restaurants(restaurant_id, philly_restaurants, similarity_matrix, n=5):\n",
    "    # Obter o índice do restaurante\n",
    "    idx = philly_restaurants.index[philly_restaurants['business_id'] == restaurant_id].tolist()[0]\n",
    "    \n",
    "    # Obter similaridade do restaurante com todos os outros\n",
    "    similars_indices = similarity_matrix[idx].argsort()[::-1]  # Do mais similar para o menos similar\n",
    "    \n",
    "    # Excluir o próprio restaurante da recomendação\n",
    "    similars_indices = similars_indices[similars_indices != idx]\n",
    "    \n",
    "    # Selecionar os n mais similares\n",
    "    similars_restaurants = philly_restaurants.iloc[similars_indices[:n]]\n",
    "    \n",
    "    return similars_restaurants[['business_id', 'name', 'categories', 'stars']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para recomendar com base nos restaurantes que o usuário já avaliou bem\n",
    "def recommend_for_user(user_id, philly_restaurants, algo, similarity_matrix, n=5):\n",
    "    # Obter os restaurantes avaliados pelo usuário\n",
    "    user_reviews = ratings[ratings['user_id'] == user_id]\n",
    "    highly_rated = user_reviews[user_reviews['stars'] >= 4]['business_id']\n",
    "    \n",
    "    # Para cada restaurante que o usuário gostou, recomendar restaurantes similares\n",
    "    recommendations = pd.DataFrame()\n",
    "    # print('highly rated ',highly_rated)\n",
    "\n",
    "    for restaurant_id in highly_rated:\n",
    "        try:\n",
    "            # Obter o índice do restaurante\n",
    "            inner_id = algo.trainset.to_inner_iid(restaurant_id)\n",
    "            \n",
    "            # Obter os restaurantes mais similares usando o modelo treinado\n",
    "            neighbors = algo.get_neighbors(inner_id, k=n)\n",
    "            # print('neighbors ',neighbors)\n",
    "            # Converter os índices internos para IDs de restaurantes\n",
    "            similar_restaurant_ids_knn = [algo.trainset.to_raw_iid(inner_id) for inner_id in neighbors]\n",
    "            \n",
    "            # Obter os detalhes dos restaurantes similares usando o modelo treinado\n",
    "            similar_restaurants_knn = philly_restaurants[philly_restaurants['business_id'].isin(similar_restaurant_ids_knn)]\n",
    "            # print('similar_restaurants_knn ',similar_restaurants_knn)\n",
    "        except ValueError:\n",
    "            # Se o restaurante não estiver no conjunto de treino, retornar um DataFrame vazio\n",
    "            similar_restaurants_knn = pd.DataFrame()\n",
    "        \n",
    "        # Obter os detalhes dos restaurantes similares usando a matriz de similaridade\n",
    "        similar_restaurants_matrix = recommend_similar_restaurants(restaurant_id, philly_restaurants, similarity_matrix, n)\n",
    "        # print('similar_restaurants_matrix ',similar_restaurants_matrix)\n",
    "        # Combinar as recomendações de ambos os métodos\n",
    "        combined_recommendations = pd.concat([similar_restaurants_knn, similar_restaurants_matrix]).drop_duplicates(subset='business_id')\n",
    "        \n",
    "        recommendations = pd.concat([recommendations, combined_recommendations])\n",
    "        # print('recommendations ',recommendations)\n",
    "    # Remover duplicatas e ordenar por popularidade (opcional: você pode melhorar o critério de ordenação)\n",
    "    recommendations = recommendations.drop_duplicates(subset='name').sort_values(by='stars', ascending=False)\n",
    "    # print('recommendations ',recommendations)\n",
    "    return recommendations['name'].head(n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_similar_users(user_id, n,similarity_matrix,user_trainset):\n",
    "    if user_id in similarity_matrix.index:\n",
    "        similar_users = similarity_matrix[user_id].sort_values(ascending=False).index[1:n+1]\n",
    "    else:\n",
    "        from sklearn.neighbors import NearestNeighbors\n",
    "        knn = NearestNeighbors(n_neighbors=n, metric='cosine')\n",
    "        knn.fit(user_trainset)\n",
    "        distances, indices = knn.kneighbors(user_trainset.loc[user_id].values.reshape(1, -1), n_neighbors=n+1)\n",
    "        similar_users = user_trainset.index[indices.flatten()][1:]\n",
    "    return similar_users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommended_restaurants(user_id, similar_users, n,user_trainset):\n",
    "    target_user_ratings = user_trainset.loc[user_id]\n",
    "    target_user_visited = target_user_ratings[target_user_ratings > 0].index\n",
    "\n",
    "    similar_users_ratings = user_trainset.loc[similar_users]\n",
    "    similar_users_ratings = similar_users_ratings.drop(columns=target_user_visited, errors='ignore')\n",
    "\n",
    "    top_rated_restaurants = similar_users_ratings.mean().sort_values(ascending=False).head(n)\n",
    "    return top_rated_restaurants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommendation system\n",
    "\n",
    "def recommend(user_id, restaurant_id, df_review_filadelfia, user_profile, restaurant_profile, type='UIBH'):\n",
    "    \n",
    "    '''\n",
    "    Esstimates rating a user gives to a restaurant\n",
    "\n",
    "    Inputs:\n",
    "    user_id - the user\n",
    "    restaurant_id - the restaurant to be rated\n",
    "    df_review_filadelfia - original df with no filtering regarding the review being positive or not\n",
    "    user_profile - df with the profiles of the users (vectors from LSA/LDA/doc2vec)\n",
    "    restaurant_profile - df with the profiles of the restaurants (vectors from LSA/LDA/doc2vec)\n",
    "    type - type of recommendations (user_item, users or items) (default = \"user_item\")\n",
    "\n",
    "    Outputs:\n",
    "    Rating\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Extracting the vector relative to the user and removing the user from the profiles\n",
    "        usr_lst = user_profile[user_profile['user_id'] == user_id].drop(columns=['user_id']).fillna(0).values\n",
    "        user_profile_function = user_profile[user_profile['user_id'] != user_id].reset_index(drop=True).fillna(0)\n",
    "        \n",
    "        # Extracting the vector relative to the restaurant and removing the restaurant from the profiles\n",
    "        bus_lst = restaurant_profile[restaurant_profile['business_id'] == restaurant_id].drop(columns=['business_id']).fillna(0).values\n",
    "        restaurant_profile_function = restaurant_profile[restaurant_profile['business_id'] != restaurant_id].reset_index(drop=True).fillna(0)\n",
    "\n",
    "\n",
    "        if type == 'UIBH':\n",
    "            # Measures the similarity between user and restaurant\n",
    "            # Rating is a linear function of the similarity\n",
    "\n",
    "            # Removing the added feature so that the vectors have the same dimensions\n",
    "            usr_lst = usr_lst[:,:-3]\n",
    "            bus_lst = bus_lst[:,:-29]\n",
    "\n",
    "            similarity_score = cosine_similarity(usr_lst, bus_lst)\n",
    "            rating = map_rating(similarity_score[0][0])\n",
    "            \n",
    "        elif type == 'UBH':\n",
    "            # Measures the similarity between the user and the other users that rated the restaurant\n",
    "            # Rating is a weighted average of the ratings given by the users (weighted by the similarity)\n",
    "\n",
    "            # Getting the other users that rated the restaurant and removing user\n",
    "            users = df_review_filadelfia[df_review_filadelfia['business_id'] == restaurant_id]['user_id']\n",
    "            users = users[users != user_id]\n",
    "\n",
    "            # Due to considering only the positive reviews for the profiles, some users don't have profile\n",
    "            # This is to remove them from the users list\n",
    "            users = users[users.isin(user_profile_function['user_id'])].unique() # and remove duplicates\n",
    "\n",
    "            # Getting the ratings given by the users and averaging if there are more than one\n",
    "            users_ratings = df_review_filadelfia[df_review_filadelfia['user_id'].isin(users)][['user_id', 'stars']]\n",
    "            users_ratings = users_ratings.groupby('user_id').mean().reset_index()\n",
    "\n",
    "            # Creating a matrix for the similar users\n",
    "            user_matrix = user_profile_function[user_profile_function['user_id'].isin(users)].drop(columns=['user_id']).values\n",
    "\n",
    "            # Similarities between user and users\n",
    "            similarity_scores = cosine_similarity(usr_lst, user_matrix)\n",
    "\n",
    "            # Transform similarities into weights\n",
    "            # This assumes that there will be other similar users.\n",
    "            # If all the other users are not similar we are giving high weights to \"not similar\" users due to this rescaling\n",
    "            similarity_scores = (similarity_scores+1)/2\n",
    "            weights = similarity_scores / np.sum(similarity_scores, axis=1)\n",
    "\n",
    "            # Computing weight-averaged rating\n",
    "            rating = np.dot(weights[0], users_ratings['stars'])\n",
    "\n",
    "        elif type == 'IBH':\n",
    "            # Measures the similarity between the restaurant and other \n",
    "            # Recommends the restaurants that are most similar to the ones the user liked before\n",
    "\n",
    "            # Getting the other restaurants the user rated and removing the restaurant\n",
    "            restaurants = df_review_filadelfia[df_review_filadelfia['user_id'] == user_id]['business_id']\n",
    "            restaurants = restaurants[restaurants != restaurant_id]\n",
    "\n",
    "            # Due to considering only the positive reviews for the profiles, some restaurants don't have profile\n",
    "            # This is to remove them from the restaurants list\n",
    "            restaurants = restaurants[restaurants.isin(restaurant_profile_function['business_id'])].unique() # and remove duplicates\n",
    "\n",
    "            # Getting the ratings given by the user and averaging if there are more than one\n",
    "            user_ratings = df_review_filadelfia[df_review_filadelfia['user_id'] == user_id][['business_id', 'stars']]\n",
    "            user_ratings = user_ratings[user_ratings['business_id'].isin(restaurants)].reset_index(drop=True)\n",
    "            user_ratings = user_ratings.groupby('business_id').mean().reset_index()\n",
    "\n",
    "            # Creating a matrix for the similar restaurants\n",
    "            restaurant_matrix = restaurant_profile_function[restaurant_profile_function['business_id'].isin(restaurants)].drop(columns=['business_id']).fillna(0).values\n",
    "\n",
    "            # Similarities between the restaurant and the other restaurants\n",
    "            similarity_scores = cosine_similarity(bus_lst, restaurant_matrix)\n",
    "\n",
    "            # Transform similarities into weights\n",
    "            # This assumes that there will be other similar restaurants.\n",
    "            # If all the other restaurants are not similar we are giving high weights to \"not similar\" restaurants due to this rescaling\n",
    "            similarity_scores = (similarity_scores+1)/2\n",
    "            weights = similarity_scores / np.sum(similarity_scores, axis=1)\n",
    "\n",
    "            # Computing weight-averaged rating\n",
    "            rating = np.dot(weights[0], user_ratings['stars'])\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Invalid type. Please choose 'UIBH', 'UBH', or 'IBH'.\")\n",
    "        \n",
    "        return rating\n",
    "    \n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(ground_truth, recommendations, k):\n",
    "\n",
    "    #top k predicted restaurants\n",
    "    top_k = recommendations[:k]\n",
    "    \n",
    "    # number of relevant items in the top-k predictions\n",
    "    relevant = sum([1 for i in top_k if i in list(ground_truth)])\n",
    "\n",
    "    return relevant / k\n",
    "\n",
    "\n",
    "def recall_at_k(ground_truth, recommendations, k):\n",
    "\n",
    "    #top k predicted restaurants\n",
    "    top_k = recommendations[:k]\n",
    "    \n",
    "    # Count the number of relevant items in the top-k predictions\n",
    "    relevant = sum([1 for i in top_k if i in list(ground_truth)])\n",
    "    \n",
    "    #number of relevant items\n",
    "    relevant_total = len(ground_truth)\n",
    "    \n",
    "    return relevant / relevant_total if relevant_total > 0 else 0\n",
    "\n",
    "def calculate_precision_recall(recommendations, ground_truth, k):\n",
    "    precision_results = []\n",
    "    recall_results = []\n",
    "\n",
    "    grouped_recommendations = recommendations.groupby('user_id')\n",
    "    grouped_ground_truth = ground_truth.groupby('user_id')\n",
    "    \n",
    "    for user_id, rec_group in grouped_recommendations:\n",
    "\n",
    "        predicted_order = rec_group['business_id'].tolist()\n",
    "        user_truth = grouped_ground_truth.get_group(user_id)['business_id']\n",
    "        user_truth_stars = grouped_ground_truth.get_group(user_id)['stars']\n",
    "        user_truth = pd.concat([user_truth, user_truth_stars], axis=1)\n",
    "        user_truth = user_truth[user_truth['stars'] > 3].reset_index(drop=True)\n",
    "        user_truth = user_truth['business_id'].tolist()\n",
    "\n",
    "        precision_k = precision_at_k(user_truth, predicted_order, k)\n",
    "        recall_k = recall_at_k(user_truth, predicted_order, k)\n",
    "        \n",
    "        precision_results.append(precision_k)\n",
    "        recall_results.append(recall_k)    \n",
    "\n",
    "    return np.mean(precision_results), np.mean(recall_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_evaluate(X_test, y_test, df_review_filadelfia, user_profile, restaurant_profile, method):\n",
    "    # predicting on the test set using the recommendations function\n",
    "    X_test['star_pred'] = X_test.apply(lambda row: recommend(row['user_id'], row['business_id'], df_review_filadelfia, user_profile, restaurant_profile, type=method), axis=1)\n",
    "    y = pd.concat([X_test['user_id'], X_test['business_id'], X_test['star_pred'], y_test], axis=1).dropna()\n",
    "\n",
    "    # Filtering the users with at least x reviews\n",
    "    user_counts = y['user_id'].value_counts()\n",
    "\n",
    "    # for metrics @3 or @5, users should have at least 10 reviews\n",
    "    users_with_10_plus_reviews = user_counts[user_counts >= 10].index\n",
    "    y_filtered = y[(y['user_id'].isin(users_with_10_plus_reviews))].reset_index(drop=True)\n",
    "\n",
    "    recommendations_3_5 = y_filtered.sort_values(by=['user_id', 'star_pred'], ascending=[True, False]).reset_index(drop=True)\n",
    "    ground_truth_3_5 = y_filtered.sort_values(by=['user_id', 'stars'], ascending=[True, False]).reset_index(drop=True)\n",
    "    # ground_truth_3_5 = ground_truth_3_5[ground_truth_3_5['stars'] > 3].reset_index(drop=True)\n",
    "\n",
    "    # for metrics @10, users should have at least 20 reviews\n",
    "    users_with_20_plus_reviews = user_counts[user_counts >= 20].index\n",
    "    y_filtered_2 = y[(y['user_id'].isin(users_with_20_plus_reviews))].reset_index(drop=True)\n",
    "\n",
    "    recommendations_10 = y_filtered_2.sort_values(by=['user_id', 'star_pred'], ascending=[True, False]).reset_index(drop=True)\n",
    "    ground_truth_10 = y_filtered_2.sort_values(by=['user_id', 'stars'], ascending=[True, False]).reset_index(drop=True)\n",
    "    # ground_truth_10 = ground_truth_10[ground_truth_10['stars'] > 3].reset_index(drop=True)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y['star_pred'], y['stars']))\n",
    "    precision_3, recall_3 = calculate_precision_recall(recommendations_3_5, ground_truth_3_5, 3)\n",
    "    precision_5, recall_5 = calculate_precision_recall(recommendations_3_5, ground_truth_3_5, 5)\n",
    "    precision_10, recall_10 = calculate_precision_recall(recommendations_10, ground_truth_10, 10)\n",
    "\n",
    "    return rmse, precision_3, recall_3, precision_5, recall_5, precision_10, recall_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "206 >= 20 reviews e 753 >= 10 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_pre_processing = ['with lemma', 'with stemma']\n",
    "# methods_feature_engineering = ['bag of words', 'word embeddings', 'lda']\n",
    "methods_feature_engineering = ['lda', 'lsa', 'doc2vec']\n",
    "# add_features_decision = ['yes','no']\n",
    "algorithms_cf = ['CF-UB','CF-IB'] #CF-IB(Colaborative Filtering Item Based),CF-UB(Colaborative Filtering User Based)\n",
    "algorithms_content = ['UBH','IBH','UIBH'] #UBH(User Based Hybrid), IBH(Item Based Hybrid), UIBH(User Item Based Hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main J\n",
    "\n",
    "def main():\n",
    "\n",
    "    metrics = pd.DataFrame(columns=['Pre-processing', 'Feature Engineering', 'Algorithm', 'RMSE', 'Precision@3', 'Recall@3', 'Precision@5', 'Recall@5', 'Precision@10', 'Recall@10'])\n",
    "\n",
    "    # Load the dataset\n",
    "    df_business_filadelfia,df_review_filadelfia,df_user_filadelfia, users_with_5_plus_reviews, restaurants_with_5_plus_reviews = load_data()\n",
    "\n",
    "    df_review_filadelfia = df_review_filadelfia.sample(10000, random_state=10).reset_index(drop=True) # TO TEST AND DELETE AFTER\n",
    "\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_review_filadelfia)\n",
    "\n",
    "    users_train = X_train['user_id']\n",
    "\n",
    "    businesses_train = X_train['business_id']\n",
    "\n",
    "    df_user_filadelfia_treino = df_user_filadelfia[df_user_filadelfia['user_id'].isin(users_train)]\n",
    "\n",
    "    df_business_filadelfia_treino = df_business_filadelfia[df_business_filadelfia['business_id'].isin(businesses_train)]\n",
    "\n",
    "    df_features_user = features_user(df_user_filadelfia_treino)\n",
    "    df_features_business = features_business(df_business_filadelfia_treino)\n",
    "\n",
    "    # Collaborative Filtering (ACHO QUE O SPLIT NÃO VAI FUNCIONAR PARA AQUI - ACHO QUE VALE A PENA TER UMA MAIN SÓ PARA O CF)\n",
    "    # for a in algorithms_cf:\n",
    "    #     if a == 'CF-UB':\n",
    "    #         algo = KNNBasic(sim_options={'user_based': True})\n",
    "    #         algo.fit(user_trainset)\n",
    "    #         predictions = algo.test(user_testset)\n",
    "\n",
    "    #     else:\n",
    "\n",
    "    #         algo = KNNBasic(sim_options={'user_based': False})\n",
    "    #         algo.fit(business_trainset)\n",
    "    #         predictions = algo.test(business_testset)  \n",
    "\n",
    "    # Content-based / Hybrid\n",
    "    for a in methods_pre_processing:\n",
    "        X_train_pre_processed = pre_processing(X_train, a)\n",
    "\n",
    "        for b in methods_feature_engineering:\n",
    "\n",
    "            if b == 'doc2vec':\n",
    "                n_components = 100\n",
    "            else:\n",
    "                n_components = 8\n",
    "\n",
    "            user_profile, restaurant_profile = profiling(X_train_pre_processed, b, n_components, users_with_5_plus_reviews, restaurants_with_5_plus_reviews)\n",
    "\n",
    "            user_profile=user_profile.merge(df_features_user,on='user_id')\n",
    "            restaurant_profile=restaurant_profile.merge(df_features_business,on='business_id')\n",
    "\n",
    "            for c in algorithms_content:\n",
    "                # return(X_test, y_test, user_profile, restaurant_profile, df_review_filadelfia)\n",
    "                rmse, precision_3, recall_3, precision_5, recall_5, precision_10, recall_10 = test_evaluate(X_test, y_test, df_review_filadelfia, user_profile, restaurant_profile, c)\n",
    "                # print(f'{a} - {b} - {c} - {rmse} - {precision_3} - {recall_3} - {precision_5} - {recall_5} - {precision_10} - {recall_10}')\n",
    "                metrics = metrics.append({'Pre-processing': a, 'Feature Engineering': b, 'Algorithm': c, 'RMSE': rmse, 'Precision@3':precision_3, 'Recall@3':recall_3, 'Precision@5':precision_5, 'Recall@5':recall_5, 'Precision@10':precision_10, 'Recall@10':recall_10}, ignore_index=True)\n",
    "                metrics.to_csv('metrics.csv')\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\helen\\AppData\\Local\\Temp\\ipykernel_18360\\3323469296.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_user_filadelfia['yelping_since'] = current_year - yelping_since.dt.year\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['business_id', 'comp_1', 'comp_2', 'comp_3', 'comp_4', 'comp_5',\n",
      "       'comp_6', 'comp_7', 'comp_8', 'stars', 'review_count', 'Monday',\n",
      "       'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday',\n",
      "       'Restaurants', 'Food', 'Nightlife', 'Bars', 'Sandwiches',\n",
      "       'American (New)', 'Pizza', 'Breakfast & Brunch',\n",
      "       'American (Traditional)', 'Coffee & Tea', 'Restaurantes',\n",
      "       'RestaurantsTakeOut', 'BusinessAcceptsCreditCards',\n",
      "       'RestaurantsDelivery', 'RestaurantsAttire_casual', 'HasTV',\n",
      "       'RestaurantsGoodForGroups', 'BikeParking', 'BusinessParking_street',\n",
      "       'GoodForKids'],\n",
      "      dtype='object')\n",
      "Index(['user_id', 'comp_1', 'comp_2', 'comp_3', 'comp_4', 'comp_5', 'comp_6',\n",
      "       'comp_7', 'comp_8', 'review_count_user', 'yelping_years',\n",
      "       'average_stars'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\helen\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\helen\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\helen\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\helen\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\helen\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\helen\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\helen\\AppData\\Local\\Temp\\ipykernel_18360\\1082773717.py:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics = metrics.append({'Pre-processing': a, 'Feature Engineering': b, 'Algorithm': c, 'RMSE': rmse, 'Precision@3':precision_3, 'Recall@3':recall_3, 'Precision@5':precision_5, 'Recall@5':recall_5, 'Precision@10':precision_10, 'Recall@10':recall_10}, ignore_index=True)\n",
      "c:\\Users\\helen\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\helen\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\helen\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\helen\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\helen\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\helen\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\helen\\AppData\\Local\\Temp\\ipykernel_18360\\1082773717.py:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics = metrics.append({'Pre-processing': a, 'Feature Engineering': b, 'Algorithm': c, 'RMSE': rmse, 'Precision@3':precision_3, 'Recall@3':recall_3, 'Precision@5':precision_5, 'Recall@5':recall_5, 'Precision@10':precision_10, 'Recall@10':recall_10}, ignore_index=True)\n",
      "c:\\Users\\helen\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\helen\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\helen\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\helen\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\helen\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\helen\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\helen\\AppData\\Local\\Temp\\ipykernel_18360\\1082773717.py:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics = metrics.append({'Pre-processing': a, 'Feature Engineering': b, 'Algorithm': c, 'RMSE': rmse, 'Precision@3':precision_3, 'Recall@3':recall_3, 'Precision@5':precision_5, 'Recall@5':recall_5, 'Precision@10':precision_10, 'Recall@10':recall_10}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['business_id', 'comp_1', 'comp_2', 'comp_3', 'comp_4', 'comp_5',\n",
      "       'comp_6', 'comp_7', 'comp_8', 'stars', 'review_count', 'Monday',\n",
      "       'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday',\n",
      "       'Restaurants', 'Food', 'Nightlife', 'Bars', 'Sandwiches',\n",
      "       'American (New)', 'Pizza', 'Breakfast & Brunch',\n",
      "       'American (Traditional)', 'Coffee & Tea', 'Restaurantes',\n",
      "       'RestaurantsTakeOut', 'BusinessAcceptsCreditCards',\n",
      "       'RestaurantsDelivery', 'RestaurantsAttire_casual', 'HasTV',\n",
      "       'RestaurantsGoodForGroups', 'BikeParking', 'BusinessParking_street',\n",
      "       'GoodForKids'],\n",
      "      dtype='object')\n",
      "Index(['user_id', 'comp_1', 'comp_2', 'comp_3', 'comp_4', 'comp_5', 'comp_6',\n",
      "       'comp_7', 'comp_8', 'review_count_user', 'yelping_years',\n",
      "       'average_stars'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "metrics_final = main()\n",
    "metrics_final.to_csv('metrics_final.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mecd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
